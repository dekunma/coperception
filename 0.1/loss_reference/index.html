
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>Loss reference - CoPerception</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.9647289d.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#coperception.utils.loss" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-component="outdated" hidden>
        <aside class="md-banner md-banner--warning">
          
        </aside>
      </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="CoPerception" class="md-header__button md-logo" aria-label="CoPerception" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CoPerception
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Loss reference
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="CoPerception" class="md-nav__button md-logo" aria-label="CoPerception" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    CoPerception
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Welcome to CoPerception
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../det_model_base_reference/" class="md-nav__link">
        Det model base reference
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Loss reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Loss reference
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss" class="md-nav__link">
    coperception.utils.loss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.BootstrappedSigmoidClassificationLoss" class="md-nav__link">
    BootstrappedSigmoidClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="BootstrappedSigmoidClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.BootstrappedSigmoidClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss" class="md-nav__link">
    Loss
  </a>
  
    <nav class="md-nav" aria-label="Loss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__" class="md-nav__link">
    __metaclass__
  </a>
  
    <nav class="md-nav" aria-label="__metaclass__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.__instancecheck__" class="md-nav__link">
    __instancecheck__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.__new__" class="md-nav__link">
    __new__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.__subclasscheck__" class="md-nav__link">
    __subclasscheck__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.register" class="md-nav__link">
    register()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.SigmoidFocalClassificationLoss" class="md-nav__link">
    SigmoidFocalClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="SigmoidFocalClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.SigmoidFocalClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.SoftmaxFocalClassificationLoss" class="md-nav__link">
    SoftmaxFocalClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="SoftmaxFocalClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.SoftmaxFocalClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedL2LocalizationLoss" class="md-nav__link">
    WeightedL2LocalizationLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSigmoidClassificationLoss" class="md-nav__link">
    WeightedSigmoidClassificationLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSmoothL1LocalizationLoss" class="md-nav__link">
    WeightedSmoothL1LocalizationLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSoftmaxClassificationLoss" class="md-nav__link">
    WeightedSoftmaxClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="WeightedSoftmaxClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSoftmaxClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.indices_to_dense_vector" class="md-nav__link">
    indices_to_dense_vector()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../v2x_sim_det_reference/" class="md-nav__link">
        V2x sim det reference
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Test
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Test" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Test
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../test/" class="md-nav__link">
        Title
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss" class="md-nav__link">
    coperception.utils.loss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.BootstrappedSigmoidClassificationLoss" class="md-nav__link">
    BootstrappedSigmoidClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="BootstrappedSigmoidClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.BootstrappedSigmoidClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss" class="md-nav__link">
    Loss
  </a>
  
    <nav class="md-nav" aria-label="Loss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__" class="md-nav__link">
    __metaclass__
  </a>
  
    <nav class="md-nav" aria-label="__metaclass__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.__instancecheck__" class="md-nav__link">
    __instancecheck__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.__new__" class="md-nav__link">
    __new__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.__subclasscheck__" class="md-nav__link">
    __subclasscheck__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__metaclass__.register" class="md-nav__link">
    register()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.Loss.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.SigmoidFocalClassificationLoss" class="md-nav__link">
    SigmoidFocalClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="SigmoidFocalClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.SigmoidFocalClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.SoftmaxFocalClassificationLoss" class="md-nav__link">
    SoftmaxFocalClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="SoftmaxFocalClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.SoftmaxFocalClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedL2LocalizationLoss" class="md-nav__link">
    WeightedL2LocalizationLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSigmoidClassificationLoss" class="md-nav__link">
    WeightedSigmoidClassificationLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSmoothL1LocalizationLoss" class="md-nav__link">
    WeightedSmoothL1LocalizationLoss
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSoftmaxClassificationLoss" class="md-nav__link">
    WeightedSoftmaxClassificationLoss
  </a>
  
    <nav class="md-nav" aria-label="WeightedSoftmaxClassificationLoss">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coperception.utils.loss.WeightedSoftmaxClassificationLoss.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coperception.utils.loss.indices_to_dense_vector" class="md-nav__link">
    indices_to_dense_vector()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


  <h1>Loss reference</h1>

<div class="doc doc-object doc-module">

<a id="coperception.utils.loss"></a>
    <div class="doc doc-contents first">

      <p>Classification and regression loss functions for object detection.</p>
<p>Localization losses:
 * WeightedL2LocalizationLoss
 * WeightedSmoothL1LocalizationLoss</p>
<p>Classification losses:
 * WeightedSigmoidClassificationLoss
 * WeightedSoftmaxClassificationLoss
 * BootstrappedSigmoidClassificationLoss</p>



  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.BootstrappedSigmoidClassificationLoss" class="doc doc-heading">
        <code>
BootstrappedSigmoidClassificationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Bootstrapped sigmoid cross entropy classification loss function.</p>
<p>This loss uses a convex combination of training labels and the current model's
predictions as training targets in the classification loss. The idea is that
as the model improves over time, its predictions can be trusted more and we
can use these predictions to mitigate the damage of noisy/incorrect labels,
because incorrect labels are likely to be eventually highly inconsistent with
other stimuli predicted to have the same label by the model.</p>
<p>In "soft" bootstrapping, we use all predicted class probabilities, whereas in
"hard" bootstrapping, we use the single class favored by the model.</p>
<p>See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by
Reed et al. (ICLR 2015).</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BootstrappedSigmoidClassificationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Bootstrapped sigmoid cross entropy classification loss function.</span>

<span class="sd">  This loss uses a convex combination of training labels and the current model&#39;s</span>
<span class="sd">  predictions as training targets in the classification loss. The idea is that</span>
<span class="sd">  as the model improves over time, its predictions can be trusted more and we</span>
<span class="sd">  can use these predictions to mitigate the damage of noisy/incorrect labels,</span>
<span class="sd">  because incorrect labels are likely to be eventually highly inconsistent with</span>
<span class="sd">  other stimuli predicted to have the same label by the model.</span>

<span class="sd">  In &quot;soft&quot; bootstrapping, we use all predicted class probabilities, whereas in</span>
<span class="sd">  &quot;hard&quot; bootstrapping, we use the single class favored by the model.</span>

<span class="sd">  See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by</span>
<span class="sd">  Reed et al. (ICLR 2015).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bootstrap_type</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      alpha: a float32 scalar tensor between 0 and 1 representing interpolation</span>
<span class="sd">        weight</span>
<span class="sd">      bootstrap_type: set to either &#39;hard&#39; or &#39;soft&#39; (default)</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if bootstrap_type is not either &#39;hard&#39; or &#39;soft&#39;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">bootstrap_type</span> <span class="o">!=</span> <span class="s1">&#39;hard&#39;</span> <span class="ow">and</span> <span class="n">bootstrap_type</span> <span class="o">!=</span> <span class="s1">&#39;soft&#39;</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unrecognized bootstrap_type: must be one of &#39;</span>
                       <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">hard</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">soft.</span><span class="se">\&#39;</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bootstrap_type</span> <span class="o">=</span> <span class="n">bootstrap_type</span>

  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing the predicted logits for each class</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing one-hot encoded classification targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors, num_classes]</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bootstrap_type</span> <span class="o">==</span> <span class="s1">&#39;soft&#39;</span><span class="p">:</span>
      <span class="n">bootstrap_target_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> <span class="n">target_tensor</span> <span class="o">+</span> <span class="p">(</span>
          <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">bootstrap_target_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">*</span> <span class="n">target_tensor</span> <span class="o">+</span> <span class="p">(</span>
          <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">per_entry_cross_ent</span> <span class="o">=</span> <span class="p">(</span><span class="n">_sigmoid_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">bootstrap_target_tensor</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">prediction_tensor</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">per_entry_cross_ent</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="coperception.utils.loss.BootstrappedSigmoidClassificationLoss.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bootstrap_type</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Constructor.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>alpha</code></td>
        <td></td>
        <td><p>a float32 scalar tensor between 0 and 1 representing interpolation
weight</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>bootstrap_type</code></td>
        <td></td>
        <td><p>set to either 'hard' or 'soft' (default)</p></td>
        <td><code>&#39;soft&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>if bootstrap_type is not either 'hard' or 'soft'</p></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">bootstrap_type</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">  Args:</span>
<span class="sd">    alpha: a float32 scalar tensor between 0 and 1 representing interpolation</span>
<span class="sd">      weight</span>
<span class="sd">    bootstrap_type: set to either &#39;hard&#39; or &#39;soft&#39; (default)</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: if bootstrap_type is not either &#39;hard&#39; or &#39;soft&#39;</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">bootstrap_type</span> <span class="o">!=</span> <span class="s1">&#39;hard&#39;</span> <span class="ow">and</span> <span class="n">bootstrap_type</span> <span class="o">!=</span> <span class="s1">&#39;soft&#39;</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unrecognized bootstrap_type: must be one of &#39;</span>
                     <span class="s1">&#39;</span><span class="se">\&#39;</span><span class="s1">hard</span><span class="se">\&#39;</span><span class="s1"> or </span><span class="se">\&#39;</span><span class="s1">soft.</span><span class="se">\&#39;</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_bootstrap_type</span> <span class="o">=</span> <span class="n">bootstrap_type</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.Loss" class="doc doc-heading">
        <code>
Loss        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Abstract base class for loss functions.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Loss</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Abstract base class for loss functions.&quot;&quot;&quot;</span>
  <span class="n">__metaclass__</span> <span class="o">=</span> <span class="n">ABCMeta</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">prediction_tensor</span><span class="p">,</span>
               <span class="n">target_tensor</span><span class="p">,</span>
               <span class="n">ignore_nan_targets</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call the loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: an N-d tensor of shape [batch, anchors, ...]</span>
<span class="sd">        representing predicted quantities.</span>
<span class="sd">      target_tensor: an N-d tensor of shape [batch, anchors, ...] representing</span>
<span class="sd">        regression or classification targets.</span>
<span class="sd">      ignore_nan_targets: whether to ignore nan targets in the loss computation.</span>
<span class="sd">        E.g. can be used if the target tensor is missing groundtruth data that</span>
<span class="sd">        shouldn&#39;t be factored into the loss.</span>
<span class="sd">      scope: Op scope name. Defaults to &#39;Loss&#39; if None.</span>
<span class="sd">      **params: Additional keyword arguments for specific implementations of</span>
<span class="sd">              the Loss.</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a tensor representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">ignore_nan_targets</span><span class="p">:</span>
      <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">target_tensor</span><span class="p">),</span>
                                <span class="n">prediction_tensor</span><span class="p">,</span>
                                <span class="n">target_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>

  <span class="nd">@abstractmethod</span>
  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Method to be overridden by implementations.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: a tensor representing predicted quantities</span>
<span class="sd">      target_tensor: a tensor representing regression or classification targets</span>
<span class="sd">      **params: Additional keyword arguments for specific implementations of</span>
<span class="sd">              the Loss.</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per</span>
<span class="sd">        anchor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h3 id="coperception.utils.loss.Loss.__metaclass__" class="doc doc-heading">
        <code>
__metaclass__            (<span title="type">type</span>)
        </code>



</h3>

    <div class="doc doc-contents ">

      <p>Metaclass for defining Abstract Base Classes (ABCs).</p>
<p>Use this metaclass to create an ABC.  An ABC can be subclassed
directly, and then acts as a mix-in class.  You can also register
unrelated concrete classes (even built-in classes) and unrelated
ABCs as 'virtual subclasses' -- these and their descendants will
be considered subclasses of the registering ABC by the built-in
issubclass() function, but the registering ABC won't show up in
their MRO (Method Resolution Order) nor will method
implementations defined by the registering ABC be callable (not
even via super()).</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ABCMeta</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Metaclass for defining Abstract Base Classes (ABCs).</span>

<span class="sd">    Use this metaclass to create an ABC.  An ABC can be subclassed</span>
<span class="sd">    directly, and then acts as a mix-in class.  You can also register</span>
<span class="sd">    unrelated concrete classes (even built-in classes) and unrelated</span>
<span class="sd">    ABCs as &#39;virtual subclasses&#39; -- these and their descendants will</span>
<span class="sd">    be considered subclasses of the registering ABC by the built-in</span>
<span class="sd">    issubclass() function, but the registering ABC won&#39;t show up in</span>
<span class="sd">    their MRO (Method Resolution Order) nor will method</span>
<span class="sd">    implementations defined by the registering ABC be callable (not</span>
<span class="sd">    even via super()).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="n">mcls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">mcls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">_abc_init</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span>

    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Register a virtual subclass of an ABC.</span>

<span class="sd">        Returns the subclass, to allow usage as a class decorator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_abc_register</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__instancecheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">instance</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Override for isinstance(instance, cls).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_abc_instancecheck</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">instance</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__subclasscheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Override for issubclass(subclass, cls).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_abc_subclasscheck</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_dump_registry</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Debug helper to print the ABC registry.&quot;&quot;&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class: </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inv. counter: </span><span class="si">{</span><span class="n">get_cache_token</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
        <span class="p">(</span><span class="n">_abc_registry</span><span class="p">,</span> <span class="n">_abc_cache</span><span class="p">,</span> <span class="n">_abc_negative_cache</span><span class="p">,</span>
         <span class="n">_abc_negative_cache_version</span><span class="p">)</span> <span class="o">=</span> <span class="n">_get_dump</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_abc_registry: </span><span class="si">{</span><span class="n">_abc_registry</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_abc_cache: </span><span class="si">{</span><span class="n">_abc_cache</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_abc_negative_cache: </span><span class="si">{</span><span class="n">_abc_negative_cache</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;_abc_negative_cache_version: </span><span class="si">{</span><span class="n">_abc_negative_cache_version</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">,</span>
              <span class="n">file</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_abc_registry_clear</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Clear the registry (for debugging or testing).&quot;&quot;&quot;</span>
        <span class="n">_reset_registry</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_abc_caches_clear</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Clear the caches (for debugging or testing).&quot;&quot;&quot;</span>
        <span class="n">_reset_caches</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h4 id="coperception.utils.loss.Loss.__metaclass__.__instancecheck__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__instancecheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">instance</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h4>

    <div class="doc doc-contents ">

      <p>Override for isinstance(instance, cls).</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__instancecheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">instance</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Override for isinstance(instance, cls).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_abc_instancecheck</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">instance</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 id="coperception.utils.loss.Loss.__metaclass__.__new__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__new__</span><span class="p">(</span><span class="n">mcls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>

    <div class="doc doc-contents ">

      <p>Create and return a new object.  See help(type) for accurate signature.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="n">mcls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">cls</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">mcls</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">_abc_init</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">cls</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 id="coperception.utils.loss.Loss.__metaclass__.__subclasscheck__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__subclasscheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h4>

    <div class="doc doc-contents ">

      <p>Override for issubclass(subclass, cls).</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__subclasscheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Override for issubclass(subclass, cls).&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_abc_subclasscheck</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 id="coperception.utils.loss.Loss.__metaclass__.register" class="doc doc-heading">
<code class="highlight language-python"><span class="n">register</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">)</span></code>


</h4>

    <div class="doc doc-contents ">

      <p>Register a virtual subclass of an ABC.</p>
<p>Returns the subclass, to allow usage as a class decorator.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Register a virtual subclass of an ABC.</span>

<span class="sd">    Returns the subclass, to allow usage as a class decorator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_abc_register</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">subclass</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>





  <div class="doc doc-object doc-method">



<h3 id="coperception.utils.loss.Loss.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="n">ignore_nan_targets</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call the loss function.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>prediction_tensor</code></td>
        <td></td>
        <td><p>an N-d tensor of shape [batch, anchors, ...]
representing predicted quantities.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_tensor</code></td>
        <td></td>
        <td><p>an N-d tensor of shape [batch, anchors, ...] representing
regression or classification targets.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>ignore_nan_targets</code></td>
        <td></td>
        <td><p>whether to ignore nan targets in the loss computation.
E.g. can be used if the target tensor is missing groundtruth data that
shouldn't be factored into the loss.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>scope</code></td>
        <td></td>
        <td><p>Op scope name. Defaults to 'Loss' if None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>**params</code></td>
        <td></td>
        <td><p>Additional keyword arguments for specific implementations of
      the Loss.</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>loss</code></td>
      <td><p>a tensor representing the value of the loss function.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">prediction_tensor</span><span class="p">,</span>
             <span class="n">target_tensor</span><span class="p">,</span>
             <span class="n">ignore_nan_targets</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="o">**</span><span class="n">params</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Call the loss function.</span>

<span class="sd">  Args:</span>
<span class="sd">    prediction_tensor: an N-d tensor of shape [batch, anchors, ...]</span>
<span class="sd">      representing predicted quantities.</span>
<span class="sd">    target_tensor: an N-d tensor of shape [batch, anchors, ...] representing</span>
<span class="sd">      regression or classification targets.</span>
<span class="sd">    ignore_nan_targets: whether to ignore nan targets in the loss computation.</span>
<span class="sd">      E.g. can be used if the target tensor is missing groundtruth data that</span>
<span class="sd">      shouldn&#39;t be factored into the loss.</span>
<span class="sd">    scope: Op scope name. Defaults to &#39;Loss&#39; if None.</span>
<span class="sd">    **params: Additional keyword arguments for specific implementations of</span>
<span class="sd">            the Loss.</span>

<span class="sd">  Returns:</span>
<span class="sd">    loss: a tensor representing the value of the loss function.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">ignore_nan_targets</span><span class="p">:</span>
    <span class="n">target_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">target_tensor</span><span class="p">),</span>
                              <span class="n">prediction_tensor</span><span class="p">,</span>
                              <span class="n">target_tensor</span><span class="p">)</span>
  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.SigmoidFocalClassificationLoss" class="doc doc-heading">
        <code>
SigmoidFocalClassificationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Sigmoid focal cross entropy loss.</p>
<p>Focal loss down-weights well classified examples and focusses on the hard
examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">SigmoidFocalClassificationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sigmoid focal cross entropy loss.</span>

<span class="sd">  Focal loss down-weights well classified examples and focusses on the hard</span>
<span class="sd">  examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      gamma: exponent of the modulating factor (1 - p_t) ^ gamma.</span>
<span class="sd">      alpha: optional alpha weighting factor to balance positives vs negatives.</span>
<span class="sd">      all_zero_negative: bool. if True, will treat all zero as background.</span>
<span class="sd">        else, will treat first label as background. only affect alpha.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>

  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">prediction_tensor</span><span class="p">,</span>
                    <span class="n">target_tensor</span><span class="p">,</span>
                    <span class="n">weights</span><span class="p">,</span>
                    <span class="n">class_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing the predicted logits for each class</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing one-hot encoded classification targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>
<span class="sd">      class_indices: (Optional) A 1-D integer tensor of class indices.</span>
<span class="sd">        If provided, computes loss only for the specified class indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors, num_classes]</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weights</span> <span class="o">*=</span> <span class="n">indices_to_dense_vector</span><span class="p">(</span><span class="n">class_indices</span><span class="p">,</span>
            <span class="n">prediction_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span>
    <span class="n">per_entry_cross_ent</span> <span class="o">=</span> <span class="p">(</span><span class="n">_sigmoid_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">target_tensor</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">prediction_tensor</span><span class="p">))</span>
    <span class="n">prediction_probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span>
    <span class="n">p_t</span> <span class="o">=</span> <span class="p">((</span><span class="n">target_tensor</span> <span class="o">*</span> <span class="n">prediction_probabilities</span><span class="p">)</span> <span class="o">+</span>
           <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target_tensor</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prediction_probabilities</span><span class="p">)))</span>
    <span class="n">modulating_factor</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">:</span>
      <span class="n">modulating_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">)</span>
    <span class="n">alpha_weight_factor</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">alpha_weight_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_tensor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">+</span>
                              <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target_tensor</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">))</span>

    <span class="n">focal_cross_entropy_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">modulating_factor</span> <span class="o">*</span> <span class="n">alpha_weight_factor</span> <span class="o">*</span>
                                <span class="n">per_entry_cross_ent</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">focal_cross_entropy_loss</span> <span class="o">*</span> <span class="n">weights</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="coperception.utils.loss.SigmoidFocalClassificationLoss.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Constructor.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>gamma</code></td>
        <td></td>
        <td><p>exponent of the modulating factor (1 - p_t) ^ gamma.</p></td>
        <td><code>2.0</code></td>
      </tr>
      <tr>
        <td><code>alpha</code></td>
        <td></td>
        <td><p>optional alpha weighting factor to balance positives vs negatives.</p></td>
        <td><code>0.25</code></td>
      </tr>
      <tr>
        <td><code>all_zero_negative</code></td>
        <td></td>
        <td><p>bool. if True, will treat all zero as background.
else, will treat first label as background. only affect alpha.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">  Args:</span>
<span class="sd">    gamma: exponent of the modulating factor (1 - p_t) ^ gamma.</span>
<span class="sd">    alpha: optional alpha weighting factor to balance positives vs negatives.</span>
<span class="sd">    all_zero_negative: bool. if True, will treat all zero as background.</span>
<span class="sd">      else, will treat first label as background. only affect alpha.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.SoftmaxFocalClassificationLoss" class="doc doc-heading">
        <code>
SoftmaxFocalClassificationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Softmax focal cross entropy loss.</p>
<p>Focal loss down-weights well classified examples and focusses on the hard
examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">SoftmaxFocalClassificationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Softmax focal cross entropy loss.</span>

<span class="sd">  Focal loss down-weights well classified examples and focusses on the hard</span>
<span class="sd">  examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      gamma: exponent of the modulating factor (1 - p_t) ^ gamma.</span>
<span class="sd">      alpha: optional alpha weighting factor to balance positives vs negatives.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>

  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">prediction_tensor</span><span class="p">,</span>
                    <span class="n">target_tensor</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">class_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing the predicted logits for each class</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing one-hot encoded classification targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>
<span class="sd">      class_indices: (Optional) A 1-D integer tensor of class indices.</span>
<span class="sd">        If provided, computes loss only for the specified class indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors, num_classes]</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1">#weights = weights.unsqueeze(2)</span>
      <span class="k">if</span> <span class="n">class_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">*=</span> <span class="n">indices_to_dense_vector</span><span class="p">(</span><span class="n">class_indices</span><span class="p">,</span>
              <span class="n">prediction_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span>
    <span class="n">per_entry_cross_ent</span> <span class="o">=</span> <span class="p">(</span><span class="n">_softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">target_tensor</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">prediction_tensor</span><span class="p">))</span>

    <span class="c1"># convert [N, num_anchors] to [N, num_anchors, num_classes]</span>
    <span class="n">per_entry_cross_ent</span> <span class="o">=</span> <span class="n">per_entry_cross_ent</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">target_tensor</span>
    <span class="n">prediction_probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">p_t</span> <span class="o">=</span> <span class="p">((</span><span class="n">target_tensor</span> <span class="o">*</span> <span class="n">prediction_probabilities</span><span class="p">)</span> <span class="o">+</span>
           <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target_tensor</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prediction_probabilities</span><span class="p">)))</span>
    <span class="n">modulating_factor</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">:</span>
      <span class="n">modulating_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span><span class="p">)</span>
    <span class="n">alpha_weight_factor</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">alpha_weight_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">target_tensor</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> 
      <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">per_entry_cross_ent</span><span class="p">),</span> 
      <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">per_entry_cross_ent</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">focal_cross_entropy_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">modulating_factor</span> <span class="o">*</span> <span class="n">alpha_weight_factor</span> <span class="o">*</span>
                                <span class="n">per_entry_cross_ent</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">focal_cross_entropy_loss</span> <span class="o">*</span> <span class="n">weights</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">focal_cross_entropy_loss</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="coperception.utils.loss.SoftmaxFocalClassificationLoss.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Constructor.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>gamma</code></td>
        <td></td>
        <td><p>exponent of the modulating factor (1 - p_t) ^ gamma.</p></td>
        <td><code>2.0</code></td>
      </tr>
      <tr>
        <td><code>alpha</code></td>
        <td></td>
        <td><p>optional alpha weighting factor to balance positives vs negatives.</p></td>
        <td><code>0.25</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">  Args:</span>
<span class="sd">    gamma: exponent of the modulating factor (1 - p_t) ^ gamma.</span>
<span class="sd">    alpha: optional alpha weighting factor to balance positives vs negatives.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.WeightedL2LocalizationLoss" class="doc doc-heading">
        <code>
WeightedL2LocalizationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>L2 localization loss function with anchorwise output support.</p>
<p>Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">WeightedL2LocalizationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;L2 localization loss function with anchorwise output support.</span>

<span class="sd">  Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">code_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">code_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">code_weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        code_size] representing the (encoded) predicted locations of objects.</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        code_size] representing the regression targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors] tensor</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">prediction_tensor</span> <span class="o">-</span> <span class="n">target_tensor</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">diff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">*</span> <span class="n">diff</span>
    <span class="n">weighted_diff</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">square_diff</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">weighted_diff</span> <span class="o">*</span> <span class="n">weighted_diff</span>
    <span class="k">return</span> <span class="n">square_diff</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.WeightedSigmoidClassificationLoss" class="doc doc-heading">
        <code>
WeightedSigmoidClassificationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Sigmoid cross entropy classification loss function.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">WeightedSigmoidClassificationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Sigmoid cross entropy classification loss function.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">prediction_tensor</span><span class="p">,</span>
                    <span class="n">target_tensor</span><span class="p">,</span>
                    <span class="n">weights</span><span class="p">,</span>
                    <span class="n">class_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing the predicted logits for each class</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing one-hot encoded classification targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>
<span class="sd">      class_indices: (Optional) A 1-D integer tensor of class indices.</span>
<span class="sd">        If provided, computes loss only for the specified class indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors, num_classes]</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">class_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">weights</span> <span class="o">*=</span> <span class="n">indices_to_dense_vector</span><span class="p">(</span><span class="n">class_indices</span><span class="p">,</span>
            <span class="n">prediction_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span>
    <span class="n">per_entry_cross_ent</span> <span class="o">=</span> <span class="p">(</span><span class="n">_sigmoid_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">target_tensor</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">prediction_tensor</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">per_entry_cross_ent</span> <span class="o">*</span> <span class="n">weights</span>
</code></pre></div>
        </details>


    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.WeightedSmoothL1LocalizationLoss" class="doc doc-heading">
        <code>
WeightedSmoothL1LocalizationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Smooth L1 localization loss function.</p>
<p>The smooth L1_loss is defined elementwise as .5 x^2 if |x|&lt;1 and |x|-.5
otherwise, where x is the difference between predictions and target.</p>
<p>See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015)</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">WeightedSmoothL1LocalizationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Smooth L1 localization loss function.</span>

<span class="sd">  The smooth L1_loss is defined elementwise as .5 x^2 if |x|&lt;1 and |x|-.5</span>
<span class="sd">  otherwise, where x is the difference between predictions and target.</span>

<span class="sd">  See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">code_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">codewise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_sigma</span> <span class="o">=</span> <span class="n">sigma</span>
    <span class="k">if</span> <span class="n">code_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">code_weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_codewise</span> <span class="o">=</span> <span class="n">codewise</span>
  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        code_size] representing the (encoded) predicted locations of objects.</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        code_size] representing the regression targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors] tensor</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">prediction_tensor</span> <span class="o">-</span> <span class="n">target_tensor</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">diff</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">code_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_code_weights</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">prediction_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">target_tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      <span class="n">diff</span> <span class="o">=</span> <span class="n">code_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>
    <span class="n">abs_diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
    <span class="n">abs_diff_lt_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">le</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">abs_diff</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">abs_diff_lt_1</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">abs_diff</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigma</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> \
      <span class="o">+</span> <span class="p">(</span><span class="n">abs_diff</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">abs_diff_lt_1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_codewise</span><span class="p">:</span>
      <span class="n">anchorwise_smooth_l1norm</span> <span class="o">=</span> <span class="n">loss</span>
      <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">anchorwise_smooth_l1norm</span> <span class="o">*=</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">anchorwise_smooth_l1norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="c1">#  * weights</span>
      <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">anchorwise_smooth_l1norm</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="k">return</span> <span class="n">anchorwise_smooth_l1norm</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">












  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="coperception.utils.loss.WeightedSoftmaxClassificationLoss" class="doc doc-heading">
        <code>
WeightedSoftmaxClassificationLoss            (<a class="autorefs autorefs-internal" title="coperception.utils.loss.Loss" href="#coperception.utils.loss.Loss">Loss</a>)
        </code>



</h2>

    <div class="doc doc-contents ">

      <p>Softmax loss function.</p>

        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">WeightedSoftmaxClassificationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Softmax loss function.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      logit_scale: When this value is high, the prediction is &quot;diffused&quot; and</span>
<span class="sd">                   when this value is low, the prediction is made peakier.</span>
<span class="sd">                   (default 1.0)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_logit_scale</span> <span class="o">=</span> <span class="n">logit_scale</span>

  <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">      prediction_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing the predicted logits for each class</span>
<span class="sd">      target_tensor: A float tensor of shape [batch_size, num_anchors,</span>
<span class="sd">        num_classes] representing one-hot encoded classification targets</span>
<span class="sd">      weights: a float tensor of shape [batch_size, num_anchors]</span>

<span class="sd">    Returns:</span>
<span class="sd">      loss: a float tensor of shape [batch_size, num_anchors]</span>
<span class="sd">        representing the value of the loss function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="n">prediction_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">prediction_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span>
        <span class="n">prediction_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logit_scale</span><span class="p">)</span>
    <span class="n">per_row_cross_ent</span> <span class="o">=</span> <span class="p">(</span><span class="n">_softmax_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">target_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span>
        <span class="n">logits</span><span class="o">=</span><span class="n">prediction_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">per_row_cross_ent</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span>
</code></pre></div>
        </details>



  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="coperception.utils.loss.WeightedSoftmaxClassificationLoss.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Constructor.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>logit_scale</code></td>
        <td></td>
        <td><p>When this value is high, the prediction is "diffused" and
           when this value is low, the prediction is made peakier.
           (default 1.0)</p></td>
        <td><code>1.0</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Constructor.</span>

<span class="sd">  Args:</span>
<span class="sd">    logit_scale: When this value is high, the prediction is &quot;diffused&quot; and</span>
<span class="sd">                 when this value is low, the prediction is made peakier.</span>
<span class="sd">                 (default 1.0)</span>

<span class="sd">  &quot;&quot;&quot;</span>
  <span class="bp">self</span><span class="o">.</span><span class="n">_logit_scale</span> <span class="o">=</span> <span class="n">logit_scale</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="coperception.utils.loss.indices_to_dense_vector" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices_to_dense_vector</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">indices_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">numpy</span><span class="o">.</span><span class="n">float32</span><span class="s1">&#39;&gt;)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Creates dense vector with indices set to specific value and rest to zeros.</p>
<p>This function exists because it is unclear if it is safe to use
  tf.sparse_to_dense(indices, [size], 1, validate_indices=False)
with indices which are not ordered.
This function accepts a dynamic size (e.g. tf.shape(tensor)[0])</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>indices</code></td>
        <td></td>
        <td><p>1d Tensor with integer indices which are to be set to
  indices_values.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>size</code></td>
        <td></td>
        <td><p>scalar with size (integer) of output Tensor.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>indices_value</code></td>
        <td></td>
        <td><p>values of elements specified by indices in the output vector</p></td>
        <td><code>1.0</code></td>
      </tr>
      <tr>
        <td><code>default_value</code></td>
        <td></td>
        <td><p>values of other elements in the output vector.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td></td>
        <td><p>data type.</p></td>
        <td><code>&lt;class &#39;numpy.float32&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td><p>dense 1D Tensor of shape [size] with indices set to indices_values and the
    rest set to default_value.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>coperception/utils/loss.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">indices_to_dense_vector</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span>
                            <span class="n">size</span><span class="p">,</span>
                            <span class="n">indices_value</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                            <span class="n">default_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates dense vector with indices set to specific value and rest to zeros.</span>

<span class="sd">  This function exists because it is unclear if it is safe to use</span>
<span class="sd">    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)</span>
<span class="sd">  with indices which are not ordered.</span>
<span class="sd">  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])</span>

<span class="sd">  Args:</span>
<span class="sd">    indices: 1d Tensor with integer indices which are to be set to</span>
<span class="sd">        indices_values.</span>
<span class="sd">    size: scalar with size (integer) of output Tensor.</span>
<span class="sd">    indices_value: values of elements specified by indices in the output vector</span>
<span class="sd">    default_value: values of other elements in the output vector.</span>
<span class="sd">    dtype: data type.</span>

<span class="sd">  Returns:</span>
<span class="sd">    dense 1D Tensor of shape [size] with indices set to indices_values and the</span>
<span class="sd">        rest set to default_value.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">default_value</span><span class="p">)</span>
  <span class="n">dense</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">indices_value</span>

  <span class="k">return</span> <span class="n">dense</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../det_model_base_reference/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Det model base reference" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Det model base reference
            </div>
          </div>
        </a>
      
      
        
        <a href="../v2x_sim_det_reference/" class="md-footer__link md-footer__link--next" aria-label="Next: V2x sim det reference" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              V2x sim det reference
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
    
  </body>
</html>