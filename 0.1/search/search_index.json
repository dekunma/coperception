{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CoPerception Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to CoPerception"},{"location":"#welcome-to-coperception","text":"","title":"Welcome to CoPerception"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"det_model_base_reference/","text":"ClassificationHead ( Module ) The classificaion head. Source code in coperception/models/det/base/DetModelBase.py class ClassificationHead ( nn . Module ): \"\"\"The classificaion head.\"\"\" def __init__ ( self , config ): super ( ClassificationHead , self ) . __init__ () category_num = config . category_num channel = 32 if config . use_map : channel += 6 if config . use_vis : channel += 13 anchor_num_per_loc = len ( config . anchor_size ) self . conv1 = nn . Conv2d ( channel , channel , kernel_size = 3 , stride = 1 , padding = 1 ) self . conv2 = nn . Conv2d ( channel , category_num * anchor_num_per_loc , kernel_size = 1 , stride = 1 , padding = 0 ) self . bn1 = nn . BatchNorm2d ( channel ) def forward ( self , x ): x = F . relu ( self . bn1 ( self . conv1 ( x ))) x = self . conv2 ( x ) return x forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in coperception/models/det/base/DetModelBase.py def forward ( self , x ): x = F . relu ( self . bn1 ( self . conv1 ( x ))) x = self . conv2 ( x ) return x DetModelBase ( Module ) Abstract class. The super class for all detection models. Attributes: Name Type Description motion_state bool To return motion state in the loss calculation method or not out_seq_len int Length of output sequence box_code_size int The specification for bounding box encoding. category_num int Number of categories. use_map bool use_map anchor_num_per_loc int Anchor number per location. classification nn.Module The classification head. regression nn.Module The regression head. agent_num int The number of agent (including RSU and vehicles) kd_flag bool Required for DiscoNet. layer int Collaborate at which layer. p_com_outage float The probability of communication outage. neighbor_feat_list list The list of neighbor features. tg_agent tensor Features of the current target agent. Source code in coperception/models/det/base/DetModelBase.py class DetModelBase ( nn . Module ): \"\"\"Abstract class. The super class for all detection models. Attributes: motion_state (bool): To return motion state in the loss calculation method or not out_seq_len (int): Length of output sequence box_code_size (int): The specification for bounding box encoding. category_num (int): Number of categories. use_map (bool): use_map anchor_num_per_loc (int): Anchor number per location. classification (nn.Module): The classification head. regression (nn.Module): The regression head. agent_num (int): The number of agent (including RSU and vehicles) kd_flag (bool): Required for DiscoNet. layer (int): Collaborate at which layer. p_com_outage (float): The probability of communication outage. neighbor_feat_list (list): The list of neighbor features. tg_agent (tensor): Features of the current target agent. \"\"\" def __init__ ( self , config , layer = 3 , in_channels = 13 , kd_flag = True , p_com_outage = 0.0 , num_agent = 5 ): super ( DetModelBase , self ) . __init__ () self . motion_state = config . motion_state self . out_seq_len = 1 if config . only_det else config . pred_len self . box_code_size = config . box_code_size self . category_num = config . category_num self . use_map = config . use_map self . anchor_num_per_loc = len ( config . anchor_size ) self . classification = ClassificationHead ( config ) self . regression = SingleRegressionHead ( config ) self . agent_num = num_agent self . kd_flag = kd_flag self . layer = layer self . p_com_outage = p_com_outage self . neighbor_feat_list = [] self . tg_agent = None def agents_to_batch ( self , feats ): \"\"\"Concatenate the features of all agents back into a bacth. Args: feats (tensor): features Returns: The concatenated feature matrix of all agents. \"\"\" feat_list = [] for i in range ( self . agent_num ): feat_list . append ( feats [:, i , :, :, :]) feat_mat = torch . cat ( tuple ( feat_list ), 0 ) return feat_mat def get_feature_maps_and_size ( self , encoded_layers : list ): \"\"\"Get the features of the collaboration layer and return the corresponding size. Args: encoded_layers (list): The output from the encoder. Returns: Feature map of the collaboration layer and the corresponding size. \"\"\" feature_maps = encoded_layers [ self . layer ] size_tuple = ( ( 1 , 32 , 256 , 256 ), ( 1 , 64 , 128 , 128 ), ( 1 , 128 , 64 , 64 ), ( 1 , 256 , 32 , 32 ), ( 1 , 512 , 16 , 16 ) ) size = size_tuple [ self . layer ] return feature_maps , size def build_feature_list ( self , batch_size : int , feat_maps ) -> list : \"\"\"Get the feature maps for each agent e.g: [10 512 16 16] -> [2 5 512 16 16] [batch size, agent num, channel, height, width] Args: batch_size (int): The batch size. feat_maps (tensor): The feature maps of the collaboration layer. Returns: A list of feature maps for each agent. \"\"\" feature_map = {} feature_list = [] for i in range ( self . agent_num ): feature_map [ i ] = torch . unsqueeze ( feat_maps [ batch_size * i : batch_size * ( i + 1 )], 1 ) feature_list . append ( feature_map [ i ]) return feature_list @staticmethod def build_local_communication_matrix ( feature_list : list ): \"\"\"Concatendate the feature list into a tensor. Args: feature_list (list): The input feature list for each agent. Returns: A tensor of concatenated features. \"\"\" return torch . cat ( tuple ( feature_list ), 1 ) def outage ( self ) -> bool : \"\"\"Simulate communication outage according to self.p_com_outage. Returns: A bool indicating if the communication outage happens. \"\"\" return np . random . choice ([ True , False ], p = [ self . p_com_outage , 1 - self . p_com_outage ]) @staticmethod def feature_transformation ( b , j , local_com_mat , all_warp , device , size ): \"\"\"Transform the features of the other agent (j) to the coordinate system of the current agent. Args: b (int): The index of the sample in current batch. j (int): The index of the other agent. local_com_mat (tensor): The local communication matrix. Features of all the agents. all_warp (tensor): The warp matrix for current sample for the current agent. device: The device used for PyTorch. size (tuple): Size of the feature map. Returns: A tensor of transformed features of agent j. \"\"\" nb_agent = torch . unsqueeze ( local_com_mat [ b , j ], 0 ) # [1 512 16 16] nb_warp = all_warp [ j ] # [4 4] # normalize the translation vector x_trans = ( 4 * nb_warp [ 0 , 3 ]) / 128 y_trans = - ( 4 * nb_warp [ 1 , 3 ]) / 128 theta_rot = torch . tensor ( [[ nb_warp [ 0 , 0 ], nb_warp [ 0 , 1 ], 0.0 ], [ nb_warp [ 1 , 0 ], nb_warp [ 1 , 1 ], 0.0 ]]) . type ( dtype = torch . float ) . to ( device ) theta_rot = torch . unsqueeze ( theta_rot , 0 ) grid_rot = F . affine_grid ( theta_rot , size = torch . Size ( size )) # get grid for grid sample theta_trans = torch . tensor ([[ 1.0 , 0.0 , x_trans ], [ 0.0 , 1.0 , y_trans ]]) . type ( dtype = torch . float ) . to ( device ) theta_trans = torch . unsqueeze ( theta_trans , 0 ) grid_trans = F . affine_grid ( theta_trans , size = torch . Size ( size )) # get grid for grid sample # first rotate the feature map, then translate it warp_feat_rot = F . grid_sample ( nb_agent , grid_rot , mode = 'bilinear' ) warp_feat_trans = F . grid_sample ( warp_feat_rot , grid_trans , mode = 'bilinear' ) return torch . squeeze ( warp_feat_trans ) def build_neighbors_feature_list ( self , b , agent_idx , all_warp , num_agent , local_com_mat , device , size ) -> None : \"\"\"Append the features of the neighbors of current agent to the neighbor_feat_list list. Args: b (int): The index of the sample in current batch. agent_idx (int): The index of the current agent. all_warp (tensor): The warp matrix for current sample for the current agent. num_agent (int): The number of agents. local_com_mat (tensor): The local communication matrix. Features of all the agents. device: The device used for PyTorch. size (tuple): Size of the feature map. \"\"\" for j in range ( num_agent ): if j != agent_idx : warp_feat = DetModelBase . feature_transformation ( b , j , local_com_mat , all_warp , device , size ) self . neighbor_feat_list . append ( warp_feat ) def get_decoded_layers ( self , encoded_layers , feature_fuse_matrix , batch_size ): \"\"\"Replace the collaboration layer of the output from the encoder with fused feature maps. Args: encoded_layers (list): The output from the encoder. feature_fuse_matrix (tensor): The fused feature maps. batch_size (int): The batch size. Returns: A list. Output from the decoder. \"\"\" encoded_layers [ self . layer ] = feature_fuse_matrix decoded_layers = self . decoder ( * encoded_layers , batch_size , kd_flag = self . kd_flag ) return decoded_layers def get_cls_loc_result ( self , x ): \"\"\"Get the classification and localization result. Args: x (tensor): The output from the last layer of the decoder. Returns: cls_preds (tensor): Predictions of the classification head. loc_preds (tensor): Predications of the localization head. result (dict): A dictionary of classificaion, localization, and optional motion state classification result. \"\"\" # Cell Classification head cls_preds = self . classification ( x ) cls_preds = cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () cls_preds = cls_preds . view ( cls_preds . shape [ 0 ], - 1 , self . category_num ) # Detection head loc_preds = self . regression ( x ) loc_preds = loc_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () loc_preds = loc_preds . view ( - 1 , loc_preds . size ( 1 ), loc_preds . size ( 2 ), self . anchor_num_per_loc , self . out_seq_len , self . box_code_size ) # loc_pred (N * T * W * H * loc) result = { 'loc' : loc_preds , 'cls' : cls_preds } # MotionState head if self . motion_state : motion_cat = 3 motion_cls_preds = self . motion_cls ( x ) motion_cls_preds = motion_cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () motion_cls_preds = motion_cls_preds . view ( cls_preds . shape [ 0 ], - 1 , motion_cat ) result [ 'state' ] = motion_cls_preds return cls_preds , loc_preds , result agents_to_batch ( self , feats ) Concatenate the features of all agents back into a bacth. Parameters: Name Type Description Default feats tensor features required Returns: Type Description The concatenated feature matrix of all agents. Source code in coperception/models/det/base/DetModelBase.py def agents_to_batch ( self , feats ): \"\"\"Concatenate the features of all agents back into a bacth. Args: feats (tensor): features Returns: The concatenated feature matrix of all agents. \"\"\" feat_list = [] for i in range ( self . agent_num ): feat_list . append ( feats [:, i , :, :, :]) feat_mat = torch . cat ( tuple ( feat_list ), 0 ) return feat_mat build_feature_list ( self , batch_size , feat_maps ) Get the feature maps for each agent e.g: [10 512 16 16] -> [2 5 512 16 16] [batch size, agent num, channel, height, width] Parameters: Name Type Description Default batch_size int The batch size. required feat_maps tensor The feature maps of the collaboration layer. required Returns: Type Description list A list of feature maps for each agent. Source code in coperception/models/det/base/DetModelBase.py def build_feature_list ( self , batch_size : int , feat_maps ) -> list : \"\"\"Get the feature maps for each agent e.g: [10 512 16 16] -> [2 5 512 16 16] [batch size, agent num, channel, height, width] Args: batch_size (int): The batch size. feat_maps (tensor): The feature maps of the collaboration layer. Returns: A list of feature maps for each agent. \"\"\" feature_map = {} feature_list = [] for i in range ( self . agent_num ): feature_map [ i ] = torch . unsqueeze ( feat_maps [ batch_size * i : batch_size * ( i + 1 )], 1 ) feature_list . append ( feature_map [ i ]) return feature_list build_local_communication_matrix ( feature_list ) staticmethod Concatendate the feature list into a tensor. Parameters: Name Type Description Default feature_list list The input feature list for each agent. required Returns: Type Description A tensor of concatenated features. Source code in coperception/models/det/base/DetModelBase.py @staticmethod def build_local_communication_matrix ( feature_list : list ): \"\"\"Concatendate the feature list into a tensor. Args: feature_list (list): The input feature list for each agent. Returns: A tensor of concatenated features. \"\"\" return torch . cat ( tuple ( feature_list ), 1 ) build_neighbors_feature_list ( self , b , agent_idx , all_warp , num_agent , local_com_mat , device , size ) Append the features of the neighbors of current agent to the neighbor_feat_list list. Parameters: Name Type Description Default b int The index of the sample in current batch. required agent_idx int The index of the current agent. required all_warp tensor The warp matrix for current sample for the current agent. required num_agent int The number of agents. required local_com_mat tensor The local communication matrix. Features of all the agents. required device The device used for PyTorch. required size tuple Size of the feature map. required Source code in coperception/models/det/base/DetModelBase.py def build_neighbors_feature_list ( self , b , agent_idx , all_warp , num_agent , local_com_mat , device , size ) -> None : \"\"\"Append the features of the neighbors of current agent to the neighbor_feat_list list. Args: b (int): The index of the sample in current batch. agent_idx (int): The index of the current agent. all_warp (tensor): The warp matrix for current sample for the current agent. num_agent (int): The number of agents. local_com_mat (tensor): The local communication matrix. Features of all the agents. device: The device used for PyTorch. size (tuple): Size of the feature map. \"\"\" for j in range ( num_agent ): if j != agent_idx : warp_feat = DetModelBase . feature_transformation ( b , j , local_com_mat , all_warp , device , size ) self . neighbor_feat_list . append ( warp_feat ) feature_transformation ( b , j , local_com_mat , all_warp , device , size ) staticmethod Transform the features of the other agent (j) to the coordinate system of the current agent. Parameters: Name Type Description Default b int The index of the sample in current batch. required j int The index of the other agent. required local_com_mat tensor The local communication matrix. Features of all the agents. required all_warp tensor The warp matrix for current sample for the current agent. required device The device used for PyTorch. required size tuple Size of the feature map. required Returns: Type Description A tensor of transformed features of agent j. Source code in coperception/models/det/base/DetModelBase.py @staticmethod def feature_transformation ( b , j , local_com_mat , all_warp , device , size ): \"\"\"Transform the features of the other agent (j) to the coordinate system of the current agent. Args: b (int): The index of the sample in current batch. j (int): The index of the other agent. local_com_mat (tensor): The local communication matrix. Features of all the agents. all_warp (tensor): The warp matrix for current sample for the current agent. device: The device used for PyTorch. size (tuple): Size of the feature map. Returns: A tensor of transformed features of agent j. \"\"\" nb_agent = torch . unsqueeze ( local_com_mat [ b , j ], 0 ) # [1 512 16 16] nb_warp = all_warp [ j ] # [4 4] # normalize the translation vector x_trans = ( 4 * nb_warp [ 0 , 3 ]) / 128 y_trans = - ( 4 * nb_warp [ 1 , 3 ]) / 128 theta_rot = torch . tensor ( [[ nb_warp [ 0 , 0 ], nb_warp [ 0 , 1 ], 0.0 ], [ nb_warp [ 1 , 0 ], nb_warp [ 1 , 1 ], 0.0 ]]) . type ( dtype = torch . float ) . to ( device ) theta_rot = torch . unsqueeze ( theta_rot , 0 ) grid_rot = F . affine_grid ( theta_rot , size = torch . Size ( size )) # get grid for grid sample theta_trans = torch . tensor ([[ 1.0 , 0.0 , x_trans ], [ 0.0 , 1.0 , y_trans ]]) . type ( dtype = torch . float ) . to ( device ) theta_trans = torch . unsqueeze ( theta_trans , 0 ) grid_trans = F . affine_grid ( theta_trans , size = torch . Size ( size )) # get grid for grid sample # first rotate the feature map, then translate it warp_feat_rot = F . grid_sample ( nb_agent , grid_rot , mode = 'bilinear' ) warp_feat_trans = F . grid_sample ( warp_feat_rot , grid_trans , mode = 'bilinear' ) return torch . squeeze ( warp_feat_trans ) get_cls_loc_result ( self , x ) Get the classification and localization result. Parameters: Name Type Description Default x tensor The output from the last layer of the decoder. required Returns: Type Description cls_preds (tensor) Predictions of the classification head. loc_preds (tensor): Predications of the localization head. result (dict): A dictionary of classificaion, localization, and optional motion state classification result. Source code in coperception/models/det/base/DetModelBase.py def get_cls_loc_result ( self , x ): \"\"\"Get the classification and localization result. Args: x (tensor): The output from the last layer of the decoder. Returns: cls_preds (tensor): Predictions of the classification head. loc_preds (tensor): Predications of the localization head. result (dict): A dictionary of classificaion, localization, and optional motion state classification result. \"\"\" # Cell Classification head cls_preds = self . classification ( x ) cls_preds = cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () cls_preds = cls_preds . view ( cls_preds . shape [ 0 ], - 1 , self . category_num ) # Detection head loc_preds = self . regression ( x ) loc_preds = loc_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () loc_preds = loc_preds . view ( - 1 , loc_preds . size ( 1 ), loc_preds . size ( 2 ), self . anchor_num_per_loc , self . out_seq_len , self . box_code_size ) # loc_pred (N * T * W * H * loc) result = { 'loc' : loc_preds , 'cls' : cls_preds } # MotionState head if self . motion_state : motion_cat = 3 motion_cls_preds = self . motion_cls ( x ) motion_cls_preds = motion_cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () motion_cls_preds = motion_cls_preds . view ( cls_preds . shape [ 0 ], - 1 , motion_cat ) result [ 'state' ] = motion_cls_preds return cls_preds , loc_preds , result get_decoded_layers ( self , encoded_layers , feature_fuse_matrix , batch_size ) Replace the collaboration layer of the output from the encoder with fused feature maps. Parameters: Name Type Description Default encoded_layers list The output from the encoder. required feature_fuse_matrix tensor The fused feature maps. required batch_size int The batch size. required Returns: Type Description A list. Output from the decoder. Source code in coperception/models/det/base/DetModelBase.py def get_decoded_layers ( self , encoded_layers , feature_fuse_matrix , batch_size ): \"\"\"Replace the collaboration layer of the output from the encoder with fused feature maps. Args: encoded_layers (list): The output from the encoder. feature_fuse_matrix (tensor): The fused feature maps. batch_size (int): The batch size. Returns: A list. Output from the decoder. \"\"\" encoded_layers [ self . layer ] = feature_fuse_matrix decoded_layers = self . decoder ( * encoded_layers , batch_size , kd_flag = self . kd_flag ) return decoded_layers get_feature_maps_and_size ( self , encoded_layers ) Get the features of the collaboration layer and return the corresponding size. Parameters: Name Type Description Default encoded_layers list The output from the encoder. required Returns: Type Description Feature map of the collaboration layer and the corresponding size. Source code in coperception/models/det/base/DetModelBase.py def get_feature_maps_and_size ( self , encoded_layers : list ): \"\"\"Get the features of the collaboration layer and return the corresponding size. Args: encoded_layers (list): The output from the encoder. Returns: Feature map of the collaboration layer and the corresponding size. \"\"\" feature_maps = encoded_layers [ self . layer ] size_tuple = ( ( 1 , 32 , 256 , 256 ), ( 1 , 64 , 128 , 128 ), ( 1 , 128 , 64 , 64 ), ( 1 , 256 , 32 , 32 ), ( 1 , 512 , 16 , 16 ) ) size = size_tuple [ self . layer ] return feature_maps , size outage ( self ) Simulate communication outage according to self.p_com_outage. Returns: Type Description bool A bool indicating if the communication outage happens. Source code in coperception/models/det/base/DetModelBase.py def outage ( self ) -> bool : \"\"\"Simulate communication outage according to self.p_com_outage. Returns: A bool indicating if the communication outage happens. \"\"\" return np . random . choice ([ True , False ], p = [ self . p_com_outage , 1 - self . p_com_outage ]) SingleRegressionHead ( Module ) The regression head. Source code in coperception/models/det/base/DetModelBase.py class SingleRegressionHead ( nn . Module ): \"\"\"The regression head.\"\"\" def __init__ ( self , config ): super ( SingleRegressionHead , self ) . __init__ () channel = 32 if config . use_map : channel += 6 if config . use_vis : channel += 13 anchor_num_per_loc = len ( config . anchor_size ) box_code_size = config . box_code_size out_seq_len = 1 if config . only_det else config . pred_len if config . binary : if config . only_det : self . box_prediction = nn . Sequential ( nn . Conv2d ( channel , channel , kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( channel ), nn . ReLU (), nn . Conv2d ( channel , anchor_num_per_loc * box_code_size * out_seq_len , kernel_size = 1 , stride = 1 , padding = 0 )) else : self . box_prediction = nn . Sequential ( nn . Conv2d ( channel , 128 , kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 ), nn . ReLU (), nn . Conv2d ( 128 , 128 , kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 ), nn . ReLU (), nn . Conv2d ( 128 , anchor_num_per_loc * box_code_size * out_seq_len , kernel_size = 1 , stride = 1 , padding = 0 )) def forward ( self , x ): box = self . box_prediction ( x ) return box forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in coperception/models/det/base/DetModelBase.py def forward ( self , x ): box = self . box_prediction ( x ) return box","title":"Det model base reference"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.ClassificationHead","text":"The classificaion head. Source code in coperception/models/det/base/DetModelBase.py class ClassificationHead ( nn . Module ): \"\"\"The classificaion head.\"\"\" def __init__ ( self , config ): super ( ClassificationHead , self ) . __init__ () category_num = config . category_num channel = 32 if config . use_map : channel += 6 if config . use_vis : channel += 13 anchor_num_per_loc = len ( config . anchor_size ) self . conv1 = nn . Conv2d ( channel , channel , kernel_size = 3 , stride = 1 , padding = 1 ) self . conv2 = nn . Conv2d ( channel , category_num * anchor_num_per_loc , kernel_size = 1 , stride = 1 , padding = 0 ) self . bn1 = nn . BatchNorm2d ( channel ) def forward ( self , x ): x = F . relu ( self . bn1 ( self . conv1 ( x ))) x = self . conv2 ( x ) return x","title":"ClassificationHead"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.ClassificationHead.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in coperception/models/det/base/DetModelBase.py def forward ( self , x ): x = F . relu ( self . bn1 ( self . conv1 ( x ))) x = self . conv2 ( x ) return x","title":"forward()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase","text":"Abstract class. The super class for all detection models. Attributes: Name Type Description motion_state bool To return motion state in the loss calculation method or not out_seq_len int Length of output sequence box_code_size int The specification for bounding box encoding. category_num int Number of categories. use_map bool use_map anchor_num_per_loc int Anchor number per location. classification nn.Module The classification head. regression nn.Module The regression head. agent_num int The number of agent (including RSU and vehicles) kd_flag bool Required for DiscoNet. layer int Collaborate at which layer. p_com_outage float The probability of communication outage. neighbor_feat_list list The list of neighbor features. tg_agent tensor Features of the current target agent. Source code in coperception/models/det/base/DetModelBase.py class DetModelBase ( nn . Module ): \"\"\"Abstract class. The super class for all detection models. Attributes: motion_state (bool): To return motion state in the loss calculation method or not out_seq_len (int): Length of output sequence box_code_size (int): The specification for bounding box encoding. category_num (int): Number of categories. use_map (bool): use_map anchor_num_per_loc (int): Anchor number per location. classification (nn.Module): The classification head. regression (nn.Module): The regression head. agent_num (int): The number of agent (including RSU and vehicles) kd_flag (bool): Required for DiscoNet. layer (int): Collaborate at which layer. p_com_outage (float): The probability of communication outage. neighbor_feat_list (list): The list of neighbor features. tg_agent (tensor): Features of the current target agent. \"\"\" def __init__ ( self , config , layer = 3 , in_channels = 13 , kd_flag = True , p_com_outage = 0.0 , num_agent = 5 ): super ( DetModelBase , self ) . __init__ () self . motion_state = config . motion_state self . out_seq_len = 1 if config . only_det else config . pred_len self . box_code_size = config . box_code_size self . category_num = config . category_num self . use_map = config . use_map self . anchor_num_per_loc = len ( config . anchor_size ) self . classification = ClassificationHead ( config ) self . regression = SingleRegressionHead ( config ) self . agent_num = num_agent self . kd_flag = kd_flag self . layer = layer self . p_com_outage = p_com_outage self . neighbor_feat_list = [] self . tg_agent = None def agents_to_batch ( self , feats ): \"\"\"Concatenate the features of all agents back into a bacth. Args: feats (tensor): features Returns: The concatenated feature matrix of all agents. \"\"\" feat_list = [] for i in range ( self . agent_num ): feat_list . append ( feats [:, i , :, :, :]) feat_mat = torch . cat ( tuple ( feat_list ), 0 ) return feat_mat def get_feature_maps_and_size ( self , encoded_layers : list ): \"\"\"Get the features of the collaboration layer and return the corresponding size. Args: encoded_layers (list): The output from the encoder. Returns: Feature map of the collaboration layer and the corresponding size. \"\"\" feature_maps = encoded_layers [ self . layer ] size_tuple = ( ( 1 , 32 , 256 , 256 ), ( 1 , 64 , 128 , 128 ), ( 1 , 128 , 64 , 64 ), ( 1 , 256 , 32 , 32 ), ( 1 , 512 , 16 , 16 ) ) size = size_tuple [ self . layer ] return feature_maps , size def build_feature_list ( self , batch_size : int , feat_maps ) -> list : \"\"\"Get the feature maps for each agent e.g: [10 512 16 16] -> [2 5 512 16 16] [batch size, agent num, channel, height, width] Args: batch_size (int): The batch size. feat_maps (tensor): The feature maps of the collaboration layer. Returns: A list of feature maps for each agent. \"\"\" feature_map = {} feature_list = [] for i in range ( self . agent_num ): feature_map [ i ] = torch . unsqueeze ( feat_maps [ batch_size * i : batch_size * ( i + 1 )], 1 ) feature_list . append ( feature_map [ i ]) return feature_list @staticmethod def build_local_communication_matrix ( feature_list : list ): \"\"\"Concatendate the feature list into a tensor. Args: feature_list (list): The input feature list for each agent. Returns: A tensor of concatenated features. \"\"\" return torch . cat ( tuple ( feature_list ), 1 ) def outage ( self ) -> bool : \"\"\"Simulate communication outage according to self.p_com_outage. Returns: A bool indicating if the communication outage happens. \"\"\" return np . random . choice ([ True , False ], p = [ self . p_com_outage , 1 - self . p_com_outage ]) @staticmethod def feature_transformation ( b , j , local_com_mat , all_warp , device , size ): \"\"\"Transform the features of the other agent (j) to the coordinate system of the current agent. Args: b (int): The index of the sample in current batch. j (int): The index of the other agent. local_com_mat (tensor): The local communication matrix. Features of all the agents. all_warp (tensor): The warp matrix for current sample for the current agent. device: The device used for PyTorch. size (tuple): Size of the feature map. Returns: A tensor of transformed features of agent j. \"\"\" nb_agent = torch . unsqueeze ( local_com_mat [ b , j ], 0 ) # [1 512 16 16] nb_warp = all_warp [ j ] # [4 4] # normalize the translation vector x_trans = ( 4 * nb_warp [ 0 , 3 ]) / 128 y_trans = - ( 4 * nb_warp [ 1 , 3 ]) / 128 theta_rot = torch . tensor ( [[ nb_warp [ 0 , 0 ], nb_warp [ 0 , 1 ], 0.0 ], [ nb_warp [ 1 , 0 ], nb_warp [ 1 , 1 ], 0.0 ]]) . type ( dtype = torch . float ) . to ( device ) theta_rot = torch . unsqueeze ( theta_rot , 0 ) grid_rot = F . affine_grid ( theta_rot , size = torch . Size ( size )) # get grid for grid sample theta_trans = torch . tensor ([[ 1.0 , 0.0 , x_trans ], [ 0.0 , 1.0 , y_trans ]]) . type ( dtype = torch . float ) . to ( device ) theta_trans = torch . unsqueeze ( theta_trans , 0 ) grid_trans = F . affine_grid ( theta_trans , size = torch . Size ( size )) # get grid for grid sample # first rotate the feature map, then translate it warp_feat_rot = F . grid_sample ( nb_agent , grid_rot , mode = 'bilinear' ) warp_feat_trans = F . grid_sample ( warp_feat_rot , grid_trans , mode = 'bilinear' ) return torch . squeeze ( warp_feat_trans ) def build_neighbors_feature_list ( self , b , agent_idx , all_warp , num_agent , local_com_mat , device , size ) -> None : \"\"\"Append the features of the neighbors of current agent to the neighbor_feat_list list. Args: b (int): The index of the sample in current batch. agent_idx (int): The index of the current agent. all_warp (tensor): The warp matrix for current sample for the current agent. num_agent (int): The number of agents. local_com_mat (tensor): The local communication matrix. Features of all the agents. device: The device used for PyTorch. size (tuple): Size of the feature map. \"\"\" for j in range ( num_agent ): if j != agent_idx : warp_feat = DetModelBase . feature_transformation ( b , j , local_com_mat , all_warp , device , size ) self . neighbor_feat_list . append ( warp_feat ) def get_decoded_layers ( self , encoded_layers , feature_fuse_matrix , batch_size ): \"\"\"Replace the collaboration layer of the output from the encoder with fused feature maps. Args: encoded_layers (list): The output from the encoder. feature_fuse_matrix (tensor): The fused feature maps. batch_size (int): The batch size. Returns: A list. Output from the decoder. \"\"\" encoded_layers [ self . layer ] = feature_fuse_matrix decoded_layers = self . decoder ( * encoded_layers , batch_size , kd_flag = self . kd_flag ) return decoded_layers def get_cls_loc_result ( self , x ): \"\"\"Get the classification and localization result. Args: x (tensor): The output from the last layer of the decoder. Returns: cls_preds (tensor): Predictions of the classification head. loc_preds (tensor): Predications of the localization head. result (dict): A dictionary of classificaion, localization, and optional motion state classification result. \"\"\" # Cell Classification head cls_preds = self . classification ( x ) cls_preds = cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () cls_preds = cls_preds . view ( cls_preds . shape [ 0 ], - 1 , self . category_num ) # Detection head loc_preds = self . regression ( x ) loc_preds = loc_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () loc_preds = loc_preds . view ( - 1 , loc_preds . size ( 1 ), loc_preds . size ( 2 ), self . anchor_num_per_loc , self . out_seq_len , self . box_code_size ) # loc_pred (N * T * W * H * loc) result = { 'loc' : loc_preds , 'cls' : cls_preds } # MotionState head if self . motion_state : motion_cat = 3 motion_cls_preds = self . motion_cls ( x ) motion_cls_preds = motion_cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () motion_cls_preds = motion_cls_preds . view ( cls_preds . shape [ 0 ], - 1 , motion_cat ) result [ 'state' ] = motion_cls_preds return cls_preds , loc_preds , result","title":"DetModelBase"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.agents_to_batch","text":"Concatenate the features of all agents back into a bacth. Parameters: Name Type Description Default feats tensor features required Returns: Type Description The concatenated feature matrix of all agents. Source code in coperception/models/det/base/DetModelBase.py def agents_to_batch ( self , feats ): \"\"\"Concatenate the features of all agents back into a bacth. Args: feats (tensor): features Returns: The concatenated feature matrix of all agents. \"\"\" feat_list = [] for i in range ( self . agent_num ): feat_list . append ( feats [:, i , :, :, :]) feat_mat = torch . cat ( tuple ( feat_list ), 0 ) return feat_mat","title":"agents_to_batch()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.build_feature_list","text":"Get the feature maps for each agent e.g: [10 512 16 16] -> [2 5 512 16 16] [batch size, agent num, channel, height, width] Parameters: Name Type Description Default batch_size int The batch size. required feat_maps tensor The feature maps of the collaboration layer. required Returns: Type Description list A list of feature maps for each agent. Source code in coperception/models/det/base/DetModelBase.py def build_feature_list ( self , batch_size : int , feat_maps ) -> list : \"\"\"Get the feature maps for each agent e.g: [10 512 16 16] -> [2 5 512 16 16] [batch size, agent num, channel, height, width] Args: batch_size (int): The batch size. feat_maps (tensor): The feature maps of the collaboration layer. Returns: A list of feature maps for each agent. \"\"\" feature_map = {} feature_list = [] for i in range ( self . agent_num ): feature_map [ i ] = torch . unsqueeze ( feat_maps [ batch_size * i : batch_size * ( i + 1 )], 1 ) feature_list . append ( feature_map [ i ]) return feature_list","title":"build_feature_list()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.build_local_communication_matrix","text":"Concatendate the feature list into a tensor. Parameters: Name Type Description Default feature_list list The input feature list for each agent. required Returns: Type Description A tensor of concatenated features. Source code in coperception/models/det/base/DetModelBase.py @staticmethod def build_local_communication_matrix ( feature_list : list ): \"\"\"Concatendate the feature list into a tensor. Args: feature_list (list): The input feature list for each agent. Returns: A tensor of concatenated features. \"\"\" return torch . cat ( tuple ( feature_list ), 1 )","title":"build_local_communication_matrix()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.build_neighbors_feature_list","text":"Append the features of the neighbors of current agent to the neighbor_feat_list list. Parameters: Name Type Description Default b int The index of the sample in current batch. required agent_idx int The index of the current agent. required all_warp tensor The warp matrix for current sample for the current agent. required num_agent int The number of agents. required local_com_mat tensor The local communication matrix. Features of all the agents. required device The device used for PyTorch. required size tuple Size of the feature map. required Source code in coperception/models/det/base/DetModelBase.py def build_neighbors_feature_list ( self , b , agent_idx , all_warp , num_agent , local_com_mat , device , size ) -> None : \"\"\"Append the features of the neighbors of current agent to the neighbor_feat_list list. Args: b (int): The index of the sample in current batch. agent_idx (int): The index of the current agent. all_warp (tensor): The warp matrix for current sample for the current agent. num_agent (int): The number of agents. local_com_mat (tensor): The local communication matrix. Features of all the agents. device: The device used for PyTorch. size (tuple): Size of the feature map. \"\"\" for j in range ( num_agent ): if j != agent_idx : warp_feat = DetModelBase . feature_transformation ( b , j , local_com_mat , all_warp , device , size ) self . neighbor_feat_list . append ( warp_feat )","title":"build_neighbors_feature_list()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.feature_transformation","text":"Transform the features of the other agent (j) to the coordinate system of the current agent. Parameters: Name Type Description Default b int The index of the sample in current batch. required j int The index of the other agent. required local_com_mat tensor The local communication matrix. Features of all the agents. required all_warp tensor The warp matrix for current sample for the current agent. required device The device used for PyTorch. required size tuple Size of the feature map. required Returns: Type Description A tensor of transformed features of agent j. Source code in coperception/models/det/base/DetModelBase.py @staticmethod def feature_transformation ( b , j , local_com_mat , all_warp , device , size ): \"\"\"Transform the features of the other agent (j) to the coordinate system of the current agent. Args: b (int): The index of the sample in current batch. j (int): The index of the other agent. local_com_mat (tensor): The local communication matrix. Features of all the agents. all_warp (tensor): The warp matrix for current sample for the current agent. device: The device used for PyTorch. size (tuple): Size of the feature map. Returns: A tensor of transformed features of agent j. \"\"\" nb_agent = torch . unsqueeze ( local_com_mat [ b , j ], 0 ) # [1 512 16 16] nb_warp = all_warp [ j ] # [4 4] # normalize the translation vector x_trans = ( 4 * nb_warp [ 0 , 3 ]) / 128 y_trans = - ( 4 * nb_warp [ 1 , 3 ]) / 128 theta_rot = torch . tensor ( [[ nb_warp [ 0 , 0 ], nb_warp [ 0 , 1 ], 0.0 ], [ nb_warp [ 1 , 0 ], nb_warp [ 1 , 1 ], 0.0 ]]) . type ( dtype = torch . float ) . to ( device ) theta_rot = torch . unsqueeze ( theta_rot , 0 ) grid_rot = F . affine_grid ( theta_rot , size = torch . Size ( size )) # get grid for grid sample theta_trans = torch . tensor ([[ 1.0 , 0.0 , x_trans ], [ 0.0 , 1.0 , y_trans ]]) . type ( dtype = torch . float ) . to ( device ) theta_trans = torch . unsqueeze ( theta_trans , 0 ) grid_trans = F . affine_grid ( theta_trans , size = torch . Size ( size )) # get grid for grid sample # first rotate the feature map, then translate it warp_feat_rot = F . grid_sample ( nb_agent , grid_rot , mode = 'bilinear' ) warp_feat_trans = F . grid_sample ( warp_feat_rot , grid_trans , mode = 'bilinear' ) return torch . squeeze ( warp_feat_trans )","title":"feature_transformation()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.get_cls_loc_result","text":"Get the classification and localization result. Parameters: Name Type Description Default x tensor The output from the last layer of the decoder. required Returns: Type Description cls_preds (tensor) Predictions of the classification head. loc_preds (tensor): Predications of the localization head. result (dict): A dictionary of classificaion, localization, and optional motion state classification result. Source code in coperception/models/det/base/DetModelBase.py def get_cls_loc_result ( self , x ): \"\"\"Get the classification and localization result. Args: x (tensor): The output from the last layer of the decoder. Returns: cls_preds (tensor): Predictions of the classification head. loc_preds (tensor): Predications of the localization head. result (dict): A dictionary of classificaion, localization, and optional motion state classification result. \"\"\" # Cell Classification head cls_preds = self . classification ( x ) cls_preds = cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () cls_preds = cls_preds . view ( cls_preds . shape [ 0 ], - 1 , self . category_num ) # Detection head loc_preds = self . regression ( x ) loc_preds = loc_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () loc_preds = loc_preds . view ( - 1 , loc_preds . size ( 1 ), loc_preds . size ( 2 ), self . anchor_num_per_loc , self . out_seq_len , self . box_code_size ) # loc_pred (N * T * W * H * loc) result = { 'loc' : loc_preds , 'cls' : cls_preds } # MotionState head if self . motion_state : motion_cat = 3 motion_cls_preds = self . motion_cls ( x ) motion_cls_preds = motion_cls_preds . permute ( 0 , 2 , 3 , 1 ) . contiguous () motion_cls_preds = motion_cls_preds . view ( cls_preds . shape [ 0 ], - 1 , motion_cat ) result [ 'state' ] = motion_cls_preds return cls_preds , loc_preds , result","title":"get_cls_loc_result()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.get_decoded_layers","text":"Replace the collaboration layer of the output from the encoder with fused feature maps. Parameters: Name Type Description Default encoded_layers list The output from the encoder. required feature_fuse_matrix tensor The fused feature maps. required batch_size int The batch size. required Returns: Type Description A list. Output from the decoder. Source code in coperception/models/det/base/DetModelBase.py def get_decoded_layers ( self , encoded_layers , feature_fuse_matrix , batch_size ): \"\"\"Replace the collaboration layer of the output from the encoder with fused feature maps. Args: encoded_layers (list): The output from the encoder. feature_fuse_matrix (tensor): The fused feature maps. batch_size (int): The batch size. Returns: A list. Output from the decoder. \"\"\" encoded_layers [ self . layer ] = feature_fuse_matrix decoded_layers = self . decoder ( * encoded_layers , batch_size , kd_flag = self . kd_flag ) return decoded_layers","title":"get_decoded_layers()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.get_feature_maps_and_size","text":"Get the features of the collaboration layer and return the corresponding size. Parameters: Name Type Description Default encoded_layers list The output from the encoder. required Returns: Type Description Feature map of the collaboration layer and the corresponding size. Source code in coperception/models/det/base/DetModelBase.py def get_feature_maps_and_size ( self , encoded_layers : list ): \"\"\"Get the features of the collaboration layer and return the corresponding size. Args: encoded_layers (list): The output from the encoder. Returns: Feature map of the collaboration layer and the corresponding size. \"\"\" feature_maps = encoded_layers [ self . layer ] size_tuple = ( ( 1 , 32 , 256 , 256 ), ( 1 , 64 , 128 , 128 ), ( 1 , 128 , 64 , 64 ), ( 1 , 256 , 32 , 32 ), ( 1 , 512 , 16 , 16 ) ) size = size_tuple [ self . layer ] return feature_maps , size","title":"get_feature_maps_and_size()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.DetModelBase.outage","text":"Simulate communication outage according to self.p_com_outage. Returns: Type Description bool A bool indicating if the communication outage happens. Source code in coperception/models/det/base/DetModelBase.py def outage ( self ) -> bool : \"\"\"Simulate communication outage according to self.p_com_outage. Returns: A bool indicating if the communication outage happens. \"\"\" return np . random . choice ([ True , False ], p = [ self . p_com_outage , 1 - self . p_com_outage ])","title":"outage()"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.SingleRegressionHead","text":"The regression head. Source code in coperception/models/det/base/DetModelBase.py class SingleRegressionHead ( nn . Module ): \"\"\"The regression head.\"\"\" def __init__ ( self , config ): super ( SingleRegressionHead , self ) . __init__ () channel = 32 if config . use_map : channel += 6 if config . use_vis : channel += 13 anchor_num_per_loc = len ( config . anchor_size ) box_code_size = config . box_code_size out_seq_len = 1 if config . only_det else config . pred_len if config . binary : if config . only_det : self . box_prediction = nn . Sequential ( nn . Conv2d ( channel , channel , kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( channel ), nn . ReLU (), nn . Conv2d ( channel , anchor_num_per_loc * box_code_size * out_seq_len , kernel_size = 1 , stride = 1 , padding = 0 )) else : self . box_prediction = nn . Sequential ( nn . Conv2d ( channel , 128 , kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 ), nn . ReLU (), nn . Conv2d ( 128 , 128 , kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm2d ( 128 ), nn . ReLU (), nn . Conv2d ( 128 , anchor_num_per_loc * box_code_size * out_seq_len , kernel_size = 1 , stride = 1 , padding = 0 )) def forward ( self , x ): box = self . box_prediction ( x ) return box","title":"SingleRegressionHead"},{"location":"det_model_base_reference/#coperception.models.det.base.DetModelBase.SingleRegressionHead.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in coperception/models/det/base/DetModelBase.py def forward ( self , x ): box = self . box_prediction ( x ) return box","title":"forward()"},{"location":"loss_reference/","text":"Classification and regression loss functions for object detection. Localization losses: * WeightedL2LocalizationLoss * WeightedSmoothL1LocalizationLoss Classification losses: * WeightedSigmoidClassificationLoss * WeightedSoftmaxClassificationLoss * BootstrappedSigmoidClassificationLoss BootstrappedSigmoidClassificationLoss ( Loss ) Bootstrapped sigmoid cross entropy classification loss function. This loss uses a convex combination of training labels and the current model's predictions as training targets in the classification loss. The idea is that as the model improves over time, its predictions can be trusted more and we can use these predictions to mitigate the damage of noisy/incorrect labels, because incorrect labels are likely to be eventually highly inconsistent with other stimuli predicted to have the same label by the model. In \"soft\" bootstrapping, we use all predicted class probabilities, whereas in \"hard\" bootstrapping, we use the single class favored by the model. See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by Reed et al. (ICLR 2015). Source code in coperception/utils/loss.py class BootstrappedSigmoidClassificationLoss ( Loss ): \"\"\"Bootstrapped sigmoid cross entropy classification loss function. This loss uses a convex combination of training labels and the current model's predictions as training targets in the classification loss. The idea is that as the model improves over time, its predictions can be trusted more and we can use these predictions to mitigate the damage of noisy/incorrect labels, because incorrect labels are likely to be eventually highly inconsistent with other stimuli predicted to have the same label by the model. In \"soft\" bootstrapping, we use all predicted class probabilities, whereas in \"hard\" bootstrapping, we use the single class favored by the model. See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by Reed et al. (ICLR 2015). \"\"\" def __init__ ( self , alpha , bootstrap_type = 'soft' ): \"\"\"Constructor. Args: alpha: a float32 scalar tensor between 0 and 1 representing interpolation weight bootstrap_type: set to either 'hard' or 'soft' (default) Raises: ValueError: if bootstrap_type is not either 'hard' or 'soft' \"\"\" if bootstrap_type != 'hard' and bootstrap_type != 'soft' : raise ValueError ( 'Unrecognized bootstrap_type: must be one of ' ' \\' hard \\' or \\' soft. \\' ' ) self . _alpha = alpha self . _bootstrap_type = bootstrap_type def _compute_loss ( self , prediction_tensor , target_tensor , weights ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" if self . _bootstrap_type == 'soft' : bootstrap_target_tensor = self . _alpha * target_tensor + ( 1.0 - self . _alpha ) * torch . sigmoid ( prediction_tensor ) else : bootstrap_target_tensor = self . _alpha * target_tensor + ( 1.0 - self . _alpha ) * ( torch . sigmoid ( prediction_tensor ) > 0.5 ) . float () per_entry_cross_ent = ( _sigmoid_cross_entropy_with_logits ( labels = bootstrap_target_tensor , logits = prediction_tensor )) return per_entry_cross_ent * weights . unsqueeze ( 2 ) __init__ ( self , alpha , bootstrap_type = 'soft' ) special Constructor. Parameters: Name Type Description Default alpha a float32 scalar tensor between 0 and 1 representing interpolation weight required bootstrap_type set to either 'hard' or 'soft' (default) 'soft' Exceptions: Type Description ValueError if bootstrap_type is not either 'hard' or 'soft' Source code in coperception/utils/loss.py def __init__ ( self , alpha , bootstrap_type = 'soft' ): \"\"\"Constructor. Args: alpha: a float32 scalar tensor between 0 and 1 representing interpolation weight bootstrap_type: set to either 'hard' or 'soft' (default) Raises: ValueError: if bootstrap_type is not either 'hard' or 'soft' \"\"\" if bootstrap_type != 'hard' and bootstrap_type != 'soft' : raise ValueError ( 'Unrecognized bootstrap_type: must be one of ' ' \\' hard \\' or \\' soft. \\' ' ) self . _alpha = alpha self . _bootstrap_type = bootstrap_type Loss Abstract base class for loss functions. Source code in coperception/utils/loss.py class Loss ( object ): \"\"\"Abstract base class for loss functions.\"\"\" __metaclass__ = ABCMeta def __call__ ( self , prediction_tensor , target_tensor , ignore_nan_targets = False , scope = None , ** params ): \"\"\"Call the loss function. Args: prediction_tensor: an N-d tensor of shape [batch, anchors, ...] representing predicted quantities. target_tensor: an N-d tensor of shape [batch, anchors, ...] representing regression or classification targets. ignore_nan_targets: whether to ignore nan targets in the loss computation. E.g. can be used if the target tensor is missing groundtruth data that shouldn't be factored into the loss. scope: Op scope name. Defaults to 'Loss' if None. **params: Additional keyword arguments for specific implementations of the Loss. Returns: loss: a tensor representing the value of the loss function. \"\"\" if ignore_nan_targets : target_tensor = torch . where ( torch . isnan ( target_tensor ), prediction_tensor , target_tensor ) return self . _compute_loss ( prediction_tensor , target_tensor , ** params ) @abstractmethod def _compute_loss ( self , prediction_tensor , target_tensor , ** params ): \"\"\"Method to be overridden by implementations. Args: prediction_tensor: a tensor representing predicted quantities target_tensor: a tensor representing regression or classification targets **params: Additional keyword arguments for specific implementations of the Loss. Returns: loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per anchor \"\"\" pass __metaclass__ ( type ) Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in coperception/utils/loss.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in coperception/utils/loss.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in coperception/utils/loss.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in coperception/utils/loss.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in coperception/utils/loss.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __call__ ( self , prediction_tensor , target_tensor , ignore_nan_targets = False , scope = None , ** params ) special Call the loss function. Parameters: Name Type Description Default prediction_tensor an N-d tensor of shape [batch, anchors, ...] representing predicted quantities. required target_tensor an N-d tensor of shape [batch, anchors, ...] representing regression or classification targets. required ignore_nan_targets whether to ignore nan targets in the loss computation. E.g. can be used if the target tensor is missing groundtruth data that shouldn't be factored into the loss. False scope Op scope name. Defaults to 'Loss' if None. None **params Additional keyword arguments for specific implementations of the Loss. {} Returns: Type Description loss a tensor representing the value of the loss function. Source code in coperception/utils/loss.py def __call__ ( self , prediction_tensor , target_tensor , ignore_nan_targets = False , scope = None , ** params ): \"\"\"Call the loss function. Args: prediction_tensor: an N-d tensor of shape [batch, anchors, ...] representing predicted quantities. target_tensor: an N-d tensor of shape [batch, anchors, ...] representing regression or classification targets. ignore_nan_targets: whether to ignore nan targets in the loss computation. E.g. can be used if the target tensor is missing groundtruth data that shouldn't be factored into the loss. scope: Op scope name. Defaults to 'Loss' if None. **params: Additional keyword arguments for specific implementations of the Loss. Returns: loss: a tensor representing the value of the loss function. \"\"\" if ignore_nan_targets : target_tensor = torch . where ( torch . isnan ( target_tensor ), prediction_tensor , target_tensor ) return self . _compute_loss ( prediction_tensor , target_tensor , ** params ) SigmoidFocalClassificationLoss ( Loss ) Sigmoid focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. Source code in coperception/utils/loss.py class SigmoidFocalClassificationLoss ( Loss ): \"\"\"Sigmoid focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. \"\"\" def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. all_zero_negative: bool. if True, will treat all zero as background. else, will treat first label as background. only affect alpha. \"\"\" self . _alpha = alpha self . _gamma = gamma def _compute_loss ( self , prediction_tensor , target_tensor , weights , class_indices = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] class_indices: (Optional) A 1-D integer tensor of class indices. If provided, computes loss only for the specified class indices. Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" weights = weights . unsqueeze ( 2 ) if class_indices is not None : weights *= indices_to_dense_vector ( class_indices , prediction_tensor . shape [ 2 ]) . view ( 1 , 1 , - 1 ) . type_as ( prediction_tensor ) per_entry_cross_ent = ( _sigmoid_cross_entropy_with_logits ( labels = target_tensor , logits = prediction_tensor )) prediction_probabilities = torch . sigmoid ( prediction_tensor ) p_t = (( target_tensor * prediction_probabilities ) + (( 1 - target_tensor ) * ( 1 - prediction_probabilities ))) modulating_factor = 1.0 if self . _gamma : modulating_factor = torch . pow ( 1.0 - p_t , self . _gamma ) alpha_weight_factor = 1.0 if self . _alpha is not None : alpha_weight_factor = ( target_tensor * self . _alpha + ( 1 - target_tensor ) * ( 1 - self . _alpha )) focal_cross_entropy_loss = ( modulating_factor * alpha_weight_factor * per_entry_cross_ent ) return focal_cross_entropy_loss * weights __init__ ( self , gamma = 2.0 , alpha = 0.25 ) special Constructor. Parameters: Name Type Description Default gamma exponent of the modulating factor (1 - p_t) ^ gamma. 2.0 alpha optional alpha weighting factor to balance positives vs negatives. 0.25 all_zero_negative bool. if True, will treat all zero as background. else, will treat first label as background. only affect alpha. required Source code in coperception/utils/loss.py def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. all_zero_negative: bool. if True, will treat all zero as background. else, will treat first label as background. only affect alpha. \"\"\" self . _alpha = alpha self . _gamma = gamma SoftmaxFocalClassificationLoss ( Loss ) Softmax focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. Source code in coperception/utils/loss.py class SoftmaxFocalClassificationLoss ( Loss ): \"\"\"Softmax focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. \"\"\" def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. \"\"\" self . _alpha = alpha self . _gamma = gamma def _compute_loss ( self , prediction_tensor , target_tensor , weights = None , class_indices = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] class_indices: (Optional) A 1-D integer tensor of class indices. If provided, computes loss only for the specified class indices. Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" if not weights is None : #weights = weights.unsqueeze(2) if class_indices is not None : weights *= indices_to_dense_vector ( class_indices , prediction_tensor . shape [ 2 ]) . view ( 1 , 1 , - 1 ) . type_as ( prediction_tensor ) per_entry_cross_ent = ( _softmax_cross_entropy_with_logits ( labels = target_tensor , logits = prediction_tensor )) # convert [N, num_anchors] to [N, num_anchors, num_classes] per_entry_cross_ent = per_entry_cross_ent . unsqueeze ( - 1 ) * target_tensor prediction_probabilities = F . softmax ( prediction_tensor , dim =- 1 ) p_t = (( target_tensor * prediction_probabilities ) + (( 1 - target_tensor ) * ( 1 - prediction_probabilities ))) modulating_factor = 1.0 if self . _gamma : modulating_factor = torch . pow ( 1.0 - p_t , self . _gamma ) alpha_weight_factor = 1.0 if self . _alpha is not None : alpha_weight_factor = torch . where ( target_tensor [ ... , 0 ] == 1 , torch . tensor ( 1 - self . _alpha ) . type_as ( per_entry_cross_ent ), torch . tensor ( self . _alpha ) . type_as ( per_entry_cross_ent )) . unsqueeze ( - 1 ) focal_cross_entropy_loss = ( modulating_factor * alpha_weight_factor * per_entry_cross_ent ) if not weights is None : return focal_cross_entropy_loss * weights else : return focal_cross_entropy_loss __init__ ( self , gamma = 2.0 , alpha = 0.25 ) special Constructor. Parameters: Name Type Description Default gamma exponent of the modulating factor (1 - p_t) ^ gamma. 2.0 alpha optional alpha weighting factor to balance positives vs negatives. 0.25 Source code in coperception/utils/loss.py def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. \"\"\" self . _alpha = alpha self . _gamma = gamma WeightedL2LocalizationLoss ( Loss ) L2 localization loss function with anchorwise output support. Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2 Source code in coperception/utils/loss.py class WeightedL2LocalizationLoss ( Loss ): \"\"\"L2 localization loss function with anchorwise output support. Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2 \"\"\" def __init__ ( self , code_weights = None ): super () . __init__ () if code_weights is not None : self . _code_weights = np . array ( code_weights , dtype = np . float32 ) self . _code_weights = torch . from_numpy ( self . _code_weights ) else : self . _code_weights = None def _compute_loss ( self , prediction_tensor , target_tensor , weights ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the (encoded) predicted locations of objects. target_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the regression targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors] tensor representing the value of the loss function. \"\"\" diff = prediction_tensor - target_tensor if self . _code_weights is not None : self . _code_weights = self . _code_weights . type_as ( prediction_tensor ) . to ( prediction_tensor . device ) self . _code_weights = self . _code_weights . view ( 1 , 1 , - 1 ) diff = self . _code_weights * diff weighted_diff = diff * weights . unsqueeze ( - 1 ) square_diff = 0.5 * weighted_diff * weighted_diff return square_diff . sum ( 2 ) WeightedSigmoidClassificationLoss ( Loss ) Sigmoid cross entropy classification loss function. Source code in coperception/utils/loss.py class WeightedSigmoidClassificationLoss ( Loss ): \"\"\"Sigmoid cross entropy classification loss function.\"\"\" def _compute_loss ( self , prediction_tensor , target_tensor , weights , class_indices = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] class_indices: (Optional) A 1-D integer tensor of class indices. If provided, computes loss only for the specified class indices. Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" weights = weights . unsqueeze ( - 1 ) if class_indices is not None : weights *= indices_to_dense_vector ( class_indices , prediction_tensor . shape [ 2 ]) . view ( 1 , 1 , - 1 ) . type_as ( prediction_tensor ) per_entry_cross_ent = ( _sigmoid_cross_entropy_with_logits ( labels = target_tensor , logits = prediction_tensor )) return per_entry_cross_ent * weights WeightedSmoothL1LocalizationLoss ( Loss ) Smooth L1 localization loss function. The smooth L1_loss is defined elementwise as .5 x^2 if |x|<1 and |x|-.5 otherwise, where x is the difference between predictions and target. See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015) Source code in coperception/utils/loss.py class WeightedSmoothL1LocalizationLoss ( Loss ): \"\"\"Smooth L1 localization loss function. The smooth L1_loss is defined elementwise as .5 x^2 if |x|<1 and |x|-.5 otherwise, where x is the difference between predictions and target. See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015) \"\"\" def __init__ ( self , sigma = 3.0 , code_weights = None , codewise = True ): super () . __init__ () self . _sigma = sigma if code_weights is not None : self . _code_weights = np . array ( code_weights , dtype = np . float32 ) self . _code_weights = torch . from_numpy ( self . _code_weights ) else : self . _code_weights = None self . _codewise = codewise def _compute_loss ( self , prediction_tensor , target_tensor , mask = None , weights = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the (encoded) predicted locations of objects. target_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the regression targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors] tensor representing the value of the loss function. \"\"\" diff = prediction_tensor - target_tensor if not mask is None : diff = diff [ mask ] if self . _code_weights is not None : code_weights = self . _code_weights . type_as ( prediction_tensor ) . to ( target_tensor . device ) diff = code_weights . view ( 1 , 1 , - 1 ) * diff abs_diff = torch . abs ( diff ) abs_diff_lt_1 = torch . le ( abs_diff , 1 / ( self . _sigma ** 2 )) . type_as ( abs_diff ) loss = abs_diff_lt_1 * 0.5 * torch . pow ( abs_diff * self . _sigma , 2 ) \\ + ( abs_diff - 0.5 / ( self . _sigma ** 2 )) * ( 1. - abs_diff_lt_1 ) if self . _codewise : anchorwise_smooth_l1norm = loss if weights is not None : anchorwise_smooth_l1norm *= weights . unsqueeze ( - 1 ) else : anchorwise_smooth_l1norm = torch . sum ( loss , 2 ) # * weights if weights is not None : anchorwise_smooth_l1norm *= weights return anchorwise_smooth_l1norm WeightedSoftmaxClassificationLoss ( Loss ) Softmax loss function. Source code in coperception/utils/loss.py class WeightedSoftmaxClassificationLoss ( Loss ): \"\"\"Softmax loss function.\"\"\" def __init__ ( self , logit_scale = 1.0 ): \"\"\"Constructor. Args: logit_scale: When this value is high, the prediction is \"diffused\" and when this value is low, the prediction is made peakier. (default 1.0) \"\"\" self . _logit_scale = logit_scale def _compute_loss ( self , prediction_tensor , target_tensor , weights ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors] representing the value of the loss function. \"\"\" num_classes = prediction_tensor . shape [ - 1 ] prediction_tensor = torch . div ( prediction_tensor , self . _logit_scale ) per_row_cross_ent = ( _softmax_cross_entropy_with_logits ( labels = target_tensor . view ( - 1 , num_classes ), logits = prediction_tensor . view ( - 1 , num_classes ))) return per_row_cross_ent . view ( weights . shape ) * weights __init__ ( self , logit_scale = 1.0 ) special Constructor. Parameters: Name Type Description Default logit_scale When this value is high, the prediction is \"diffused\" and when this value is low, the prediction is made peakier. (default 1.0) 1.0 Source code in coperception/utils/loss.py def __init__ ( self , logit_scale = 1.0 ): \"\"\"Constructor. Args: logit_scale: When this value is high, the prediction is \"diffused\" and when this value is low, the prediction is made peakier. (default 1.0) \"\"\" self . _logit_scale = logit_scale indices_to_dense_vector ( indices , size , indices_value = 1.0 , default_value = 0 , dtype =< class ' numpy . float32 '>) Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Parameters: Name Type Description Default indices 1d Tensor with integer indices which are to be set to indices_values. required size scalar with size (integer) of output Tensor. required indices_value values of elements specified by indices in the output vector 1.0 default_value values of other elements in the output vector. 0 dtype data type. <class 'numpy.float32'> Returns: Type Description dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. Source code in coperception/utils/loss.py def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = np . float32 ): \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Args: indices: 1d Tensor with integer indices which are to be set to indices_values. size: scalar with size (integer) of output Tensor. indices_value: values of elements specified by indices in the output vector default_value: values of other elements in the output vector. dtype: data type. Returns: dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" dense = torch . zeros ( size ) . fill_ ( default_value ) dense [ indices ] = indices_value return dense","title":"Loss reference"},{"location":"loss_reference/#coperception.utils.loss.BootstrappedSigmoidClassificationLoss","text":"Bootstrapped sigmoid cross entropy classification loss function. This loss uses a convex combination of training labels and the current model's predictions as training targets in the classification loss. The idea is that as the model improves over time, its predictions can be trusted more and we can use these predictions to mitigate the damage of noisy/incorrect labels, because incorrect labels are likely to be eventually highly inconsistent with other stimuli predicted to have the same label by the model. In \"soft\" bootstrapping, we use all predicted class probabilities, whereas in \"hard\" bootstrapping, we use the single class favored by the model. See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by Reed et al. (ICLR 2015). Source code in coperception/utils/loss.py class BootstrappedSigmoidClassificationLoss ( Loss ): \"\"\"Bootstrapped sigmoid cross entropy classification loss function. This loss uses a convex combination of training labels and the current model's predictions as training targets in the classification loss. The idea is that as the model improves over time, its predictions can be trusted more and we can use these predictions to mitigate the damage of noisy/incorrect labels, because incorrect labels are likely to be eventually highly inconsistent with other stimuli predicted to have the same label by the model. In \"soft\" bootstrapping, we use all predicted class probabilities, whereas in \"hard\" bootstrapping, we use the single class favored by the model. See also Training Deep Neural Networks On Noisy Labels with Bootstrapping by Reed et al. (ICLR 2015). \"\"\" def __init__ ( self , alpha , bootstrap_type = 'soft' ): \"\"\"Constructor. Args: alpha: a float32 scalar tensor between 0 and 1 representing interpolation weight bootstrap_type: set to either 'hard' or 'soft' (default) Raises: ValueError: if bootstrap_type is not either 'hard' or 'soft' \"\"\" if bootstrap_type != 'hard' and bootstrap_type != 'soft' : raise ValueError ( 'Unrecognized bootstrap_type: must be one of ' ' \\' hard \\' or \\' soft. \\' ' ) self . _alpha = alpha self . _bootstrap_type = bootstrap_type def _compute_loss ( self , prediction_tensor , target_tensor , weights ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" if self . _bootstrap_type == 'soft' : bootstrap_target_tensor = self . _alpha * target_tensor + ( 1.0 - self . _alpha ) * torch . sigmoid ( prediction_tensor ) else : bootstrap_target_tensor = self . _alpha * target_tensor + ( 1.0 - self . _alpha ) * ( torch . sigmoid ( prediction_tensor ) > 0.5 ) . float () per_entry_cross_ent = ( _sigmoid_cross_entropy_with_logits ( labels = bootstrap_target_tensor , logits = prediction_tensor )) return per_entry_cross_ent * weights . unsqueeze ( 2 )","title":"BootstrappedSigmoidClassificationLoss"},{"location":"loss_reference/#coperception.utils.loss.BootstrappedSigmoidClassificationLoss.__init__","text":"Constructor. Parameters: Name Type Description Default alpha a float32 scalar tensor between 0 and 1 representing interpolation weight required bootstrap_type set to either 'hard' or 'soft' (default) 'soft' Exceptions: Type Description ValueError if bootstrap_type is not either 'hard' or 'soft' Source code in coperception/utils/loss.py def __init__ ( self , alpha , bootstrap_type = 'soft' ): \"\"\"Constructor. Args: alpha: a float32 scalar tensor between 0 and 1 representing interpolation weight bootstrap_type: set to either 'hard' or 'soft' (default) Raises: ValueError: if bootstrap_type is not either 'hard' or 'soft' \"\"\" if bootstrap_type != 'hard' and bootstrap_type != 'soft' : raise ValueError ( 'Unrecognized bootstrap_type: must be one of ' ' \\' hard \\' or \\' soft. \\' ' ) self . _alpha = alpha self . _bootstrap_type = bootstrap_type","title":"__init__()"},{"location":"loss_reference/#coperception.utils.loss.Loss","text":"Abstract base class for loss functions. Source code in coperception/utils/loss.py class Loss ( object ): \"\"\"Abstract base class for loss functions.\"\"\" __metaclass__ = ABCMeta def __call__ ( self , prediction_tensor , target_tensor , ignore_nan_targets = False , scope = None , ** params ): \"\"\"Call the loss function. Args: prediction_tensor: an N-d tensor of shape [batch, anchors, ...] representing predicted quantities. target_tensor: an N-d tensor of shape [batch, anchors, ...] representing regression or classification targets. ignore_nan_targets: whether to ignore nan targets in the loss computation. E.g. can be used if the target tensor is missing groundtruth data that shouldn't be factored into the loss. scope: Op scope name. Defaults to 'Loss' if None. **params: Additional keyword arguments for specific implementations of the Loss. Returns: loss: a tensor representing the value of the loss function. \"\"\" if ignore_nan_targets : target_tensor = torch . where ( torch . isnan ( target_tensor ), prediction_tensor , target_tensor ) return self . _compute_loss ( prediction_tensor , target_tensor , ** params ) @abstractmethod def _compute_loss ( self , prediction_tensor , target_tensor , ** params ): \"\"\"Method to be overridden by implementations. Args: prediction_tensor: a tensor representing predicted quantities target_tensor: a tensor representing regression or classification targets **params: Additional keyword arguments for specific implementations of the Loss. Returns: loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per anchor \"\"\" pass","title":"Loss"},{"location":"loss_reference/#coperception.utils.loss.Loss.__metaclass__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in coperception/utils/loss.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__metaclass__"},{"location":"loss_reference/#coperception.utils.loss.Loss.__metaclass__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in coperception/utils/loss.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"loss_reference/#coperception.utils.loss.Loss.__metaclass__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in coperception/utils/loss.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"loss_reference/#coperception.utils.loss.Loss.__metaclass__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in coperception/utils/loss.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"loss_reference/#coperception.utils.loss.Loss.__metaclass__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in coperception/utils/loss.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"loss_reference/#coperception.utils.loss.Loss.__call__","text":"Call the loss function. Parameters: Name Type Description Default prediction_tensor an N-d tensor of shape [batch, anchors, ...] representing predicted quantities. required target_tensor an N-d tensor of shape [batch, anchors, ...] representing regression or classification targets. required ignore_nan_targets whether to ignore nan targets in the loss computation. E.g. can be used if the target tensor is missing groundtruth data that shouldn't be factored into the loss. False scope Op scope name. Defaults to 'Loss' if None. None **params Additional keyword arguments for specific implementations of the Loss. {} Returns: Type Description loss a tensor representing the value of the loss function. Source code in coperception/utils/loss.py def __call__ ( self , prediction_tensor , target_tensor , ignore_nan_targets = False , scope = None , ** params ): \"\"\"Call the loss function. Args: prediction_tensor: an N-d tensor of shape [batch, anchors, ...] representing predicted quantities. target_tensor: an N-d tensor of shape [batch, anchors, ...] representing regression or classification targets. ignore_nan_targets: whether to ignore nan targets in the loss computation. E.g. can be used if the target tensor is missing groundtruth data that shouldn't be factored into the loss. scope: Op scope name. Defaults to 'Loss' if None. **params: Additional keyword arguments for specific implementations of the Loss. Returns: loss: a tensor representing the value of the loss function. \"\"\" if ignore_nan_targets : target_tensor = torch . where ( torch . isnan ( target_tensor ), prediction_tensor , target_tensor ) return self . _compute_loss ( prediction_tensor , target_tensor , ** params )","title":"__call__()"},{"location":"loss_reference/#coperception.utils.loss.SigmoidFocalClassificationLoss","text":"Sigmoid focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. Source code in coperception/utils/loss.py class SigmoidFocalClassificationLoss ( Loss ): \"\"\"Sigmoid focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. \"\"\" def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. all_zero_negative: bool. if True, will treat all zero as background. else, will treat first label as background. only affect alpha. \"\"\" self . _alpha = alpha self . _gamma = gamma def _compute_loss ( self , prediction_tensor , target_tensor , weights , class_indices = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] class_indices: (Optional) A 1-D integer tensor of class indices. If provided, computes loss only for the specified class indices. Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" weights = weights . unsqueeze ( 2 ) if class_indices is not None : weights *= indices_to_dense_vector ( class_indices , prediction_tensor . shape [ 2 ]) . view ( 1 , 1 , - 1 ) . type_as ( prediction_tensor ) per_entry_cross_ent = ( _sigmoid_cross_entropy_with_logits ( labels = target_tensor , logits = prediction_tensor )) prediction_probabilities = torch . sigmoid ( prediction_tensor ) p_t = (( target_tensor * prediction_probabilities ) + (( 1 - target_tensor ) * ( 1 - prediction_probabilities ))) modulating_factor = 1.0 if self . _gamma : modulating_factor = torch . pow ( 1.0 - p_t , self . _gamma ) alpha_weight_factor = 1.0 if self . _alpha is not None : alpha_weight_factor = ( target_tensor * self . _alpha + ( 1 - target_tensor ) * ( 1 - self . _alpha )) focal_cross_entropy_loss = ( modulating_factor * alpha_weight_factor * per_entry_cross_ent ) return focal_cross_entropy_loss * weights","title":"SigmoidFocalClassificationLoss"},{"location":"loss_reference/#coperception.utils.loss.SigmoidFocalClassificationLoss.__init__","text":"Constructor. Parameters: Name Type Description Default gamma exponent of the modulating factor (1 - p_t) ^ gamma. 2.0 alpha optional alpha weighting factor to balance positives vs negatives. 0.25 all_zero_negative bool. if True, will treat all zero as background. else, will treat first label as background. only affect alpha. required Source code in coperception/utils/loss.py def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. all_zero_negative: bool. if True, will treat all zero as background. else, will treat first label as background. only affect alpha. \"\"\" self . _alpha = alpha self . _gamma = gamma","title":"__init__()"},{"location":"loss_reference/#coperception.utils.loss.SoftmaxFocalClassificationLoss","text":"Softmax focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. Source code in coperception/utils/loss.py class SoftmaxFocalClassificationLoss ( Loss ): \"\"\"Softmax focal cross entropy loss. Focal loss down-weights well classified examples and focusses on the hard examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition. \"\"\" def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. \"\"\" self . _alpha = alpha self . _gamma = gamma def _compute_loss ( self , prediction_tensor , target_tensor , weights = None , class_indices = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] class_indices: (Optional) A 1-D integer tensor of class indices. If provided, computes loss only for the specified class indices. Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" if not weights is None : #weights = weights.unsqueeze(2) if class_indices is not None : weights *= indices_to_dense_vector ( class_indices , prediction_tensor . shape [ 2 ]) . view ( 1 , 1 , - 1 ) . type_as ( prediction_tensor ) per_entry_cross_ent = ( _softmax_cross_entropy_with_logits ( labels = target_tensor , logits = prediction_tensor )) # convert [N, num_anchors] to [N, num_anchors, num_classes] per_entry_cross_ent = per_entry_cross_ent . unsqueeze ( - 1 ) * target_tensor prediction_probabilities = F . softmax ( prediction_tensor , dim =- 1 ) p_t = (( target_tensor * prediction_probabilities ) + (( 1 - target_tensor ) * ( 1 - prediction_probabilities ))) modulating_factor = 1.0 if self . _gamma : modulating_factor = torch . pow ( 1.0 - p_t , self . _gamma ) alpha_weight_factor = 1.0 if self . _alpha is not None : alpha_weight_factor = torch . where ( target_tensor [ ... , 0 ] == 1 , torch . tensor ( 1 - self . _alpha ) . type_as ( per_entry_cross_ent ), torch . tensor ( self . _alpha ) . type_as ( per_entry_cross_ent )) . unsqueeze ( - 1 ) focal_cross_entropy_loss = ( modulating_factor * alpha_weight_factor * per_entry_cross_ent ) if not weights is None : return focal_cross_entropy_loss * weights else : return focal_cross_entropy_loss","title":"SoftmaxFocalClassificationLoss"},{"location":"loss_reference/#coperception.utils.loss.SoftmaxFocalClassificationLoss.__init__","text":"Constructor. Parameters: Name Type Description Default gamma exponent of the modulating factor (1 - p_t) ^ gamma. 2.0 alpha optional alpha weighting factor to balance positives vs negatives. 0.25 Source code in coperception/utils/loss.py def __init__ ( self , gamma = 2.0 , alpha = 0.25 ): \"\"\"Constructor. Args: gamma: exponent of the modulating factor (1 - p_t) ^ gamma. alpha: optional alpha weighting factor to balance positives vs negatives. \"\"\" self . _alpha = alpha self . _gamma = gamma","title":"__init__()"},{"location":"loss_reference/#coperception.utils.loss.WeightedL2LocalizationLoss","text":"L2 localization loss function with anchorwise output support. Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2 Source code in coperception/utils/loss.py class WeightedL2LocalizationLoss ( Loss ): \"\"\"L2 localization loss function with anchorwise output support. Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2 \"\"\" def __init__ ( self , code_weights = None ): super () . __init__ () if code_weights is not None : self . _code_weights = np . array ( code_weights , dtype = np . float32 ) self . _code_weights = torch . from_numpy ( self . _code_weights ) else : self . _code_weights = None def _compute_loss ( self , prediction_tensor , target_tensor , weights ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the (encoded) predicted locations of objects. target_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the regression targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors] tensor representing the value of the loss function. \"\"\" diff = prediction_tensor - target_tensor if self . _code_weights is not None : self . _code_weights = self . _code_weights . type_as ( prediction_tensor ) . to ( prediction_tensor . device ) self . _code_weights = self . _code_weights . view ( 1 , 1 , - 1 ) diff = self . _code_weights * diff weighted_diff = diff * weights . unsqueeze ( - 1 ) square_diff = 0.5 * weighted_diff * weighted_diff return square_diff . sum ( 2 )","title":"WeightedL2LocalizationLoss"},{"location":"loss_reference/#coperception.utils.loss.WeightedSigmoidClassificationLoss","text":"Sigmoid cross entropy classification loss function. Source code in coperception/utils/loss.py class WeightedSigmoidClassificationLoss ( Loss ): \"\"\"Sigmoid cross entropy classification loss function.\"\"\" def _compute_loss ( self , prediction_tensor , target_tensor , weights , class_indices = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] class_indices: (Optional) A 1-D integer tensor of class indices. If provided, computes loss only for the specified class indices. Returns: loss: a float tensor of shape [batch_size, num_anchors, num_classes] representing the value of the loss function. \"\"\" weights = weights . unsqueeze ( - 1 ) if class_indices is not None : weights *= indices_to_dense_vector ( class_indices , prediction_tensor . shape [ 2 ]) . view ( 1 , 1 , - 1 ) . type_as ( prediction_tensor ) per_entry_cross_ent = ( _sigmoid_cross_entropy_with_logits ( labels = target_tensor , logits = prediction_tensor )) return per_entry_cross_ent * weights","title":"WeightedSigmoidClassificationLoss"},{"location":"loss_reference/#coperception.utils.loss.WeightedSmoothL1LocalizationLoss","text":"Smooth L1 localization loss function. The smooth L1_loss is defined elementwise as .5 x^2 if |x|<1 and |x|-.5 otherwise, where x is the difference between predictions and target. See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015) Source code in coperception/utils/loss.py class WeightedSmoothL1LocalizationLoss ( Loss ): \"\"\"Smooth L1 localization loss function. The smooth L1_loss is defined elementwise as .5 x^2 if |x|<1 and |x|-.5 otherwise, where x is the difference between predictions and target. See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015) \"\"\" def __init__ ( self , sigma = 3.0 , code_weights = None , codewise = True ): super () . __init__ () self . _sigma = sigma if code_weights is not None : self . _code_weights = np . array ( code_weights , dtype = np . float32 ) self . _code_weights = torch . from_numpy ( self . _code_weights ) else : self . _code_weights = None self . _codewise = codewise def _compute_loss ( self , prediction_tensor , target_tensor , mask = None , weights = None ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the (encoded) predicted locations of objects. target_tensor: A float tensor of shape [batch_size, num_anchors, code_size] representing the regression targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors] tensor representing the value of the loss function. \"\"\" diff = prediction_tensor - target_tensor if not mask is None : diff = diff [ mask ] if self . _code_weights is not None : code_weights = self . _code_weights . type_as ( prediction_tensor ) . to ( target_tensor . device ) diff = code_weights . view ( 1 , 1 , - 1 ) * diff abs_diff = torch . abs ( diff ) abs_diff_lt_1 = torch . le ( abs_diff , 1 / ( self . _sigma ** 2 )) . type_as ( abs_diff ) loss = abs_diff_lt_1 * 0.5 * torch . pow ( abs_diff * self . _sigma , 2 ) \\ + ( abs_diff - 0.5 / ( self . _sigma ** 2 )) * ( 1. - abs_diff_lt_1 ) if self . _codewise : anchorwise_smooth_l1norm = loss if weights is not None : anchorwise_smooth_l1norm *= weights . unsqueeze ( - 1 ) else : anchorwise_smooth_l1norm = torch . sum ( loss , 2 ) # * weights if weights is not None : anchorwise_smooth_l1norm *= weights return anchorwise_smooth_l1norm","title":"WeightedSmoothL1LocalizationLoss"},{"location":"loss_reference/#coperception.utils.loss.WeightedSoftmaxClassificationLoss","text":"Softmax loss function. Source code in coperception/utils/loss.py class WeightedSoftmaxClassificationLoss ( Loss ): \"\"\"Softmax loss function.\"\"\" def __init__ ( self , logit_scale = 1.0 ): \"\"\"Constructor. Args: logit_scale: When this value is high, the prediction is \"diffused\" and when this value is low, the prediction is made peakier. (default 1.0) \"\"\" self . _logit_scale = logit_scale def _compute_loss ( self , prediction_tensor , target_tensor , weights ): \"\"\"Compute loss function. Args: prediction_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing the predicted logits for each class target_tensor: A float tensor of shape [batch_size, num_anchors, num_classes] representing one-hot encoded classification targets weights: a float tensor of shape [batch_size, num_anchors] Returns: loss: a float tensor of shape [batch_size, num_anchors] representing the value of the loss function. \"\"\" num_classes = prediction_tensor . shape [ - 1 ] prediction_tensor = torch . div ( prediction_tensor , self . _logit_scale ) per_row_cross_ent = ( _softmax_cross_entropy_with_logits ( labels = target_tensor . view ( - 1 , num_classes ), logits = prediction_tensor . view ( - 1 , num_classes ))) return per_row_cross_ent . view ( weights . shape ) * weights","title":"WeightedSoftmaxClassificationLoss"},{"location":"loss_reference/#coperception.utils.loss.WeightedSoftmaxClassificationLoss.__init__","text":"Constructor. Parameters: Name Type Description Default logit_scale When this value is high, the prediction is \"diffused\" and when this value is low, the prediction is made peakier. (default 1.0) 1.0 Source code in coperception/utils/loss.py def __init__ ( self , logit_scale = 1.0 ): \"\"\"Constructor. Args: logit_scale: When this value is high, the prediction is \"diffused\" and when this value is low, the prediction is made peakier. (default 1.0) \"\"\" self . _logit_scale = logit_scale","title":"__init__()"},{"location":"loss_reference/#coperception.utils.loss.indices_to_dense_vector","text":"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Parameters: Name Type Description Default indices 1d Tensor with integer indices which are to be set to indices_values. required size scalar with size (integer) of output Tensor. required indices_value values of elements specified by indices in the output vector 1.0 default_value values of other elements in the output vector. 0 dtype data type. <class 'numpy.float32'> Returns: Type Description dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. Source code in coperception/utils/loss.py def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = np . float32 ): \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Args: indices: 1d Tensor with integer indices which are to be set to indices_values. size: scalar with size (integer) of output Tensor. indices_value: values of elements specified by indices in the output vector default_value: values of other elements in the output vector. dtype: data type. Returns: dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" dense = torch . zeros ( size ) . fill_ ( default_value ) dense [ indices ] = indices_value return dense","title":"indices_to_dense_vector()"},{"location":"v2x_sim_det_reference/","text":"NuscenesDataset ( Dataset ) Source code in coperception/datasets/V2XSimDet.py class NuscenesDataset ( Dataset ): def __init__ ( self , dataset_root = None , config = None , split = None , cache_size = 10000 , val = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis # dataset_root = dataset_root + '/'+split if dataset_root is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_root = dataset_root seq_dirs = [ os . path . join ( self . dataset_root , d ) for d in os . listdir ( self . dataset_root ) if os . path . isdir ( os . path . join ( self . dataset_root , d ))] seq_dirs = sorted ( seq_dirs ) self . seq_files = [ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))] self . num_sample_seqs = len ( self . seq_files ) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) ''' # For training, the size of dataset should be 17065 * 2; for validation: 1623; for testing: 4309 if split == 'train' and self.num_sample_seqs != 17065 * 2: warnings.warn(\">> The size of training dataset is not 17065 * 2.\\n\") elif split == 'val' and self.num_sample_seqs != 1623: warnings.warn(\">> The size of validation dataset is not 1719.\\n\") elif split == 'test' and self.num_sample_seqs != 4309: warnings.warn('>> The size of test dataset is not 4309.\\n') ''' # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), \\ int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = manager . dict () self . cache_size = cache_size if split == 'train' else 0 # self.cache_size = cache_size def __len__ ( self ): return self . num_sample_seqs def get_one_hot ( self , label , category_num ): one_hot_label = np . zeros (( label . shape [ 0 ], category_num )) for i in range ( label . shape [ 0 ]): one_hot_label [ i ][ label [ i ]] = 1 return one_hot_label def __getitem__ ( self , idx ): if idx in self . cache : gt_dict = self . cache [ idx ] else : seq_file = self . seq_files [ idx ] gt_data_handle = np . load ( seq_file , allow_pickle = True ) gt_dict = gt_data_handle . item () if len ( self . cache ) < self . cache_size : self . cache [ idx ] = gt_dict allocation_mask = gt_dict [ 'allocation_mask' ] . astype ( np . bool ) reg_loss_mask = gt_dict [ 'reg_loss_mask' ] . astype ( np . bool ) gt_max_iou = gt_dict [ 'gt_max_iou' ] motion_one_hot = np . zeros ( 5 ) motion_mask = np . zeros ( 5 ) # load regression target reg_target_sparse = gt_dict [ 'reg_target_sparse' ] # need to be modified Yiqi , only use reg_target and allocation_map reg_target = np . zeros ( self . reg_target_shape ) . astype ( reg_target_sparse . dtype ) reg_target [ allocation_mask ] = reg_target_sparse reg_target [ np . bitwise_not ( reg_loss_mask )] = 0 label_sparse = gt_dict [ 'label_sparse' ] one_hot_label_sparse = self . get_one_hot ( label_sparse , self . category_num ) label_one_hot = np . zeros ( self . label_one_hot_shape ) label_one_hot [:, :, :, 0 ] = 1 label_one_hot [ allocation_mask ] = one_hot_label_sparse if self . config . motion_state : motion_sparse = gt_dict [ 'motion_state' ] motion_one_hot_label_sparse = self . get_one_hot ( motion_sparse , 3 ) motion_one_hot = np . zeros ( self . label_one_hot_shape [: - 1 ] + ( 3 ,)) motion_one_hot [:, :, :, 0 ] = 1 motion_one_hot [ allocation_mask ] = motion_one_hot_label_sparse motion_mask = ( motion_one_hot [:, :, :, 2 ] == 1 ) if self . only_det : reg_target = reg_target [:, :, :, : 1 ] reg_loss_mask = reg_loss_mask [:, :, :, : 1 ] # only center for pred elif self . config . pred_type in [ 'motion' , 'center' ]: reg_loss_mask = np . expand_dims ( reg_loss_mask , axis =- 1 ) reg_loss_mask = np . repeat ( reg_loss_mask , self . box_code_size , axis =- 1 ) reg_loss_mask [:, :, :, 1 :, 2 :] = False if self . config . use_map : if ( 'map_allocation_0' in gt_dict . keys ()) or ( 'map_allocation' in gt_dict . keys ()): semantic_maps = [] for m_id in range ( self . config . map_channel ): map_alloc = gt_dict [ 'map_allocation_' + str ( m_id )] map_sparse = gt_dict [ 'map_sparse_' + str ( m_id )] recover = np . zeros ( tuple ( self . config . map_dims [: 2 ])) recover [ map_alloc ] = map_sparse recover = np . rot90 ( recover , 3 ) # recover_map = cv2.resize(recover,(self.config.map_dims[0],self.config.map_dims[1])) semantic_maps . append ( recover ) semantic_maps = np . asarray ( semantic_maps ) else : semantic_maps = np . zeros ( 0 ) ''' if self.binary: reg_target = np.concatenate([reg_target[:,:,:2],reg_target[:,:,5:]],axis=2) reg_loss_mask = np.concatenate([reg_loss_mask[:,:,:2],reg_loss_mask[:,:,5:]],axis=2) label_one_hot = np.concatenate([label_one_hot[:,:,:2],label_one_hot[:,:,5:]],axis=2) ''' padded_voxel_points = list () for i in range ( self . num_past_pcs ): indices = gt_dict [ 'voxel_indices_' + str ( i )] curr_voxels = np . zeros ( self . dims , dtype = bool ) curr_voxels [ indices [:, 0 ], indices [:, 1 ], indices [:, 2 ]] = 1 curr_voxels = np . rot90 ( curr_voxels , 3 ) padded_voxel_points . append ( curr_voxels ) padded_voxel_points = np . stack ( padded_voxel_points , 0 ) . astype ( np . float32 ) anchors_map = self . anchors_map ''' if self.binary: anchors_map = np.concatenate([anchors_map[:,:,:2],anchors_map[:,:,5:]],axis=2) ''' if self . config . use_vis : vis_maps = np . zeros ( ( self . num_past_pcs , self . config . map_dims [ - 1 ], self . config . map_dims [ 0 ], self . config . map_dims [ 1 ])) vis_free_indices = gt_dict [ 'vis_free_indices' ] vis_occupy_indices = gt_dict [ 'vis_occupy_indices' ] vis_maps [ vis_occupy_indices [ 0 , :], vis_occupy_indices [ 1 , :], vis_occupy_indices [ 2 , :], vis_occupy_indices [ 3 , :]] = math . log ( 0.7 / ( 1 - 0.7 )) vis_maps [ vis_free_indices [ 0 , :], vis_free_indices [ 1 , :], vis_free_indices [ 2 , :], vis_free_indices [ 3 , :]] = math . log ( 0.4 / ( 1 - 0.4 )) vis_maps = np . swapaxes ( vis_maps , 2 , 3 ) vis_maps = np . transpose ( vis_maps , ( 0 , 2 , 3 , 1 )) for v_id in range ( vis_maps . shape [ 0 ]): vis_maps [ v_id ] = np . rot90 ( vis_maps [ v_id ], 3 ) vis_maps = vis_maps [ - 1 ] else : vis_maps = np . zeros ( 0 ) padded_voxel_points = padded_voxel_points . astype ( np . float32 ) label_one_hot = label_one_hot . astype ( np . float32 ) reg_target = reg_target . astype ( np . float32 ) anchors_map = anchors_map . astype ( np . float32 ) motion_one_hot = motion_one_hot . astype ( np . float32 ) semantic_maps = semantic_maps . astype ( np . float32 ) vis_maps = vis_maps . astype ( np . float32 ) if self . val : return padded_voxel_points , label_one_hot , \\ reg_target , reg_loss_mask , anchors_map , motion_one_hot , motion_mask , vis_maps , [ { \"gt_box\" : gt_max_iou }], [ seq_file ] else : return padded_voxel_points , label_one_hot , \\ reg_target , reg_loss_mask , anchors_map , motion_one_hot , motion_mask , vis_maps __init__ ( self , dataset_root = None , config = None , split = None , cache_size = 10000 , val = False ) special This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) Source code in coperception/datasets/V2XSimDet.py def __init__ ( self , dataset_root = None , config = None , split = None , cache_size = 10000 , val = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis # dataset_root = dataset_root + '/'+split if dataset_root is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_root = dataset_root seq_dirs = [ os . path . join ( self . dataset_root , d ) for d in os . listdir ( self . dataset_root ) if os . path . isdir ( os . path . join ( self . dataset_root , d ))] seq_dirs = sorted ( seq_dirs ) self . seq_files = [ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))] self . num_sample_seqs = len ( self . seq_files ) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) ''' # For training, the size of dataset should be 17065 * 2; for validation: 1623; for testing: 4309 if split == 'train' and self.num_sample_seqs != 17065 * 2: warnings.warn(\">> The size of training dataset is not 17065 * 2.\\n\") elif split == 'val' and self.num_sample_seqs != 1623: warnings.warn(\">> The size of validation dataset is not 1719.\\n\") elif split == 'test' and self.num_sample_seqs != 4309: warnings.warn('>> The size of test dataset is not 4309.\\n') ''' # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), \\ int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = manager . dict () self . cache_size = cache_size if split == 'train' else 0 # self.cache_size = cache_size V2XSimDet ( Dataset ) Source code in coperception/datasets/V2XSimDet.py class V2XSimDet ( Dataset ): def __init__ ( self , dataset_roots = None , config = None , config_global = None , split = None , cache_size = 10000 , val = False , bound = None , kd_flag = False , no_cross_road = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis self . bound = bound self . kd_flag = kd_flag self . no_cross_road = no_cross_road # dataset_root = dataset_root + '/'+split if dataset_roots is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_roots = dataset_roots self . num_agent = len ( dataset_roots ) self . seq_files = [] self . seq_scenes = [] for dataset_root in self . dataset_roots : # sort directories dir_list = [ d . split ( '_' ) for d in os . listdir ( dataset_root )] dir_list . sort ( key = lambda x : ( int ( x [ 0 ]), int ( x [ 1 ]))) self . seq_scenes . append ([ int ( s [ 0 ]) for s in dir_list ]) # which scene this frame belongs to (required for visualization) dir_list = [ '_' . join ( x ) for x in dir_list ] seq_dirs = [ os . path . join ( dataset_root , d ) for d in dir_list if os . path . isdir ( os . path . join ( dataset_root , d ))] self . seq_files . append ([ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))]) self . num_sample_seqs = len ( self . seq_files [ 0 ]) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = [ manager . dict () for _ in range ( self . num_agent )] self . cache_size = cache_size if split == 'train' else 0 if self . val : self . voxel_size_global = config_global . voxel_size self . area_extents_global = config_global . area_extents self . pred_len_global = config_global . pred_len self . box_code_size_global = config_global . box_code_size self . anchor_size_global = config_global . anchor_size # object information self . anchors_map_global = init_anchors_no_check ( self . area_extents_global , self . voxel_size_global , self . box_code_size_global , self . anchor_size_global ) self . map_dims_global = [ int (( self . area_extents_global [ 0 ][ 1 ] - self . area_extents_global [ 0 ][ 0 ]) / self . voxel_size_global [ 0 ]), \\ int (( self . area_extents_global [ 1 ][ 1 ] - self . area_extents_global [ 1 ][ 0 ]) / self . voxel_size_global [ 1 ])] self . reg_target_shape_global = ( self . map_dims_global [ 0 ], self . map_dims_global [ 1 ], len ( self . anchor_size_global ), self . pred_len_global , self . box_code_size_global ) self . dims_global = config_global . map_dims self . get_meta () def get_meta ( self ): meta = NuscenesDataset ( dataset_root = self . dataset_roots [ 0 ], split = self . split , config = self . config , val = self . val ) if not self . val : self . padded_voxel_points_meta , self . label_one_hot_meta , self . reg_target_meta , self . reg_loss_mask_meta , \\ self . anchors_map_meta , _ , _ , self . vis_maps_meta = meta [ 0 ] else : self . padded_voxel_points_meta , self . label_one_hot_meta , self . reg_target_meta , self . reg_loss_mask_meta , \\ self . anchors_map_meta , _ , _ , self . vis_maps_meta , _ , _ = meta [ 0 ] del meta def __len__ ( self ): return self . num_sample_seqs def get_one_hot ( self , label , category_num ): one_hot_label = np . zeros (( label . shape [ 0 ], category_num )) for i in range ( label . shape [ 0 ]): one_hot_label [ i ][ label [ i ]] = 1 return one_hot_label def pick_single_agent ( self , agent_id , idx ): empty_flag = False if idx in self . cache [ agent_id ]: gt_dict = self . cache [ agent_id ][ idx ] else : seq_file = self . seq_files [ agent_id ][ idx ] gt_data_handle = np . load ( seq_file , allow_pickle = True ) if gt_data_handle == 0 : empty_flag = True padded_voxel_points = [] padded_voxel_points_teacher = [] label_one_hot = np . zeros_like ( self . label_one_hot_meta ) reg_target = np . zeros_like ( self . reg_target_meta ) anchors_map = np . zeros_like ( self . anchors_map_meta ) vis_maps = np . zeros_like ( self . vis_maps_meta ) reg_loss_mask = np . zeros_like ( self . reg_loss_mask_meta ) if self . bound == 'lowerbound' : padded_voxel_points = np . zeros_like ( self . padded_voxel_points_meta ) if self . kd_flag or self . bound == 'upperbound' : padded_voxel_points_teacher = np . zeros_like ( self . padded_voxel_points_meta ) if self . val : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , \\ reg_target , reg_loss_mask , anchors_map , vis_maps , [ { \"gt_box\" : []}], [ seq_file ], \\ 0 , 0 , np . zeros (( self . num_agent , 4 , 4 )) else : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , reg_target , reg_loss_mask , anchors_map , vis_maps , 0 , 0 , np . zeros ( ( self . num_agent , 4 , 4 )) else : gt_dict = gt_data_handle . item () if len ( self . cache [ agent_id ]) < self . cache_size : self . cache [ agent_id ][ idx ] = gt_dict if empty_flag == False : allocation_mask = gt_dict [ 'allocation_mask' ] . astype ( bool ) reg_loss_mask = gt_dict [ 'reg_loss_mask' ] . astype ( bool ) gt_max_iou = gt_dict [ 'gt_max_iou' ] # load regression target reg_target_sparse = gt_dict [ 'reg_target_sparse' ] # need to be modified Yiqi , only use reg_target and allocation_map reg_target = np . zeros ( self . reg_target_shape ) . astype ( reg_target_sparse . dtype ) reg_target [ allocation_mask ] = reg_target_sparse reg_target [ np . bitwise_not ( reg_loss_mask )] = 0 label_sparse = gt_dict [ 'label_sparse' ] one_hot_label_sparse = self . get_one_hot ( label_sparse , self . category_num ) label_one_hot = np . zeros ( self . label_one_hot_shape ) label_one_hot [:, :, :, 0 ] = 1 label_one_hot [ allocation_mask ] = one_hot_label_sparse if self . only_det : reg_target = reg_target [:, :, :, : 1 ] reg_loss_mask = reg_loss_mask [:, :, :, : 1 ] # only center for pred elif self . config . pred_type in [ 'motion' , 'center' ]: reg_loss_mask = np . expand_dims ( reg_loss_mask , axis =- 1 ) reg_loss_mask = np . repeat ( reg_loss_mask , self . box_code_size , axis =- 1 ) reg_loss_mask [:, :, :, 1 :, 2 :] = False # Prepare padded_voxel_points padded_voxel_points = [] if self . bound == 'lowerbound' or self . bound == 'both' : for i in range ( self . num_past_pcs ): indices = gt_dict [ 'voxel_indices_' + str ( i )] curr_voxels = np . zeros ( self . dims , dtype = bool ) curr_voxels [ indices [:, 0 ], indices [:, 1 ], indices [:, 2 ]] = 1 curr_voxels = np . rot90 ( curr_voxels , 3 ) padded_voxel_points . append ( curr_voxels ) padded_voxel_points = np . stack ( padded_voxel_points , 0 ) . astype ( np . float32 ) padded_voxel_points = padded_voxel_points . astype ( np . float32 ) anchors_map = self . anchors_map if self . config . use_vis : vis_maps = np . zeros ( ( self . num_past_pcs , self . config . map_dims [ - 1 ], self . config . map_dims [ 0 ], self . config . map_dims [ 1 ])) vis_free_indices = gt_dict [ 'vis_free_indices' ] vis_occupy_indices = gt_dict [ 'vis_occupy_indices' ] vis_maps [ vis_occupy_indices [ 0 , :], vis_occupy_indices [ 1 , :], vis_occupy_indices [ 2 , :], vis_occupy_indices [ 3 , :]] = math . log ( 0.7 / ( 1 - 0.7 )) vis_maps [ vis_free_indices [ 0 , :], vis_free_indices [ 1 , :], vis_free_indices [ 2 , :], vis_free_indices [ 3 , :]] = math . log ( 0.4 / ( 1 - 0.4 )) vis_maps = np . swapaxes ( vis_maps , 2 , 3 ) vis_maps = np . transpose ( vis_maps , ( 0 , 2 , 3 , 1 )) for v_id in range ( vis_maps . shape [ 0 ]): vis_maps [ v_id ] = np . rot90 ( vis_maps [ v_id ], 3 ) vis_maps = vis_maps [ - 1 ] else : vis_maps = np . zeros ( 0 ) if self . no_cross_road : trans_matrices = gt_dict [ 'trans_matrices_no_cross_road' ] else : trans_matrices = gt_dict [ 'trans_matrices' ] label_one_hot = label_one_hot . astype ( np . float32 ) reg_target = reg_target . astype ( np . float32 ) anchors_map = anchors_map . astype ( np . float32 ) vis_maps = vis_maps . astype ( np . float32 ) target_agent_id = gt_dict [ 'target_agent_id' ] num_sensor = gt_dict [ 'num_sensor' ] # Prepare padded_voxel_points_teacher padded_voxel_points_teacher = [] if 'voxel_indices_teacher' in gt_dict and ( self . kd_flag or self . bound == 'upperbound' or self . bound == 'both' ): if self . no_cross_road : indices_teacher = gt_dict [ 'voxel_indices_teacher_no_cross_road' ] else : indices_teacher = gt_dict [ 'voxel_indices_teacher' ] curr_voxels_teacher = np . zeros ( self . dims , dtype = bool ) curr_voxels_teacher [ indices_teacher [:, 0 ], indices_teacher [:, 1 ], indices_teacher [:, 2 ]] = 1 curr_voxels_teacher = np . rot90 ( curr_voxels_teacher , 3 ) padded_voxel_points_teacher . append ( curr_voxels_teacher ) padded_voxel_points_teacher = np . stack ( padded_voxel_points_teacher , 0 ) . astype ( np . float32 ) padded_voxel_points_teacher = padded_voxel_points_teacher . astype ( np . float32 ) if self . val : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , reg_target , reg_loss_mask , anchors_map , vis_maps , [ { \"gt_box\" : gt_max_iou }], [ seq_file ], \\ target_agent_id , num_sensor , trans_matrices else : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , reg_target , reg_loss_mask , anchors_map , vis_maps , \\ target_agent_id , num_sensor , trans_matrices def __getitem__ ( self , idx ): res = [] for i in range ( self . num_agent ): res . append ( self . pick_single_agent ( i , idx )) return res __init__ ( self , dataset_roots = None , config = None , config_global = None , split = None , cache_size = 10000 , val = False , bound = None , kd_flag = False , no_cross_road = False ) special This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) Source code in coperception/datasets/V2XSimDet.py def __init__ ( self , dataset_roots = None , config = None , config_global = None , split = None , cache_size = 10000 , val = False , bound = None , kd_flag = False , no_cross_road = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis self . bound = bound self . kd_flag = kd_flag self . no_cross_road = no_cross_road # dataset_root = dataset_root + '/'+split if dataset_roots is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_roots = dataset_roots self . num_agent = len ( dataset_roots ) self . seq_files = [] self . seq_scenes = [] for dataset_root in self . dataset_roots : # sort directories dir_list = [ d . split ( '_' ) for d in os . listdir ( dataset_root )] dir_list . sort ( key = lambda x : ( int ( x [ 0 ]), int ( x [ 1 ]))) self . seq_scenes . append ([ int ( s [ 0 ]) for s in dir_list ]) # which scene this frame belongs to (required for visualization) dir_list = [ '_' . join ( x ) for x in dir_list ] seq_dirs = [ os . path . join ( dataset_root , d ) for d in dir_list if os . path . isdir ( os . path . join ( dataset_root , d ))] self . seq_files . append ([ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))]) self . num_sample_seqs = len ( self . seq_files [ 0 ]) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = [ manager . dict () for _ in range ( self . num_agent )] self . cache_size = cache_size if split == 'train' else 0 if self . val : self . voxel_size_global = config_global . voxel_size self . area_extents_global = config_global . area_extents self . pred_len_global = config_global . pred_len self . box_code_size_global = config_global . box_code_size self . anchor_size_global = config_global . anchor_size # object information self . anchors_map_global = init_anchors_no_check ( self . area_extents_global , self . voxel_size_global , self . box_code_size_global , self . anchor_size_global ) self . map_dims_global = [ int (( self . area_extents_global [ 0 ][ 1 ] - self . area_extents_global [ 0 ][ 0 ]) / self . voxel_size_global [ 0 ]), \\ int (( self . area_extents_global [ 1 ][ 1 ] - self . area_extents_global [ 1 ][ 0 ]) / self . voxel_size_global [ 1 ])] self . reg_target_shape_global = ( self . map_dims_global [ 0 ], self . map_dims_global [ 1 ], len ( self . anchor_size_global ), self . pred_len_global , self . box_code_size_global ) self . dims_global = config_global . map_dims self . get_meta ()","title":"V2x sim det reference"},{"location":"v2x_sim_det_reference/#coperception.datasets.V2XSimDet.NuscenesDataset","text":"Source code in coperception/datasets/V2XSimDet.py class NuscenesDataset ( Dataset ): def __init__ ( self , dataset_root = None , config = None , split = None , cache_size = 10000 , val = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis # dataset_root = dataset_root + '/'+split if dataset_root is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_root = dataset_root seq_dirs = [ os . path . join ( self . dataset_root , d ) for d in os . listdir ( self . dataset_root ) if os . path . isdir ( os . path . join ( self . dataset_root , d ))] seq_dirs = sorted ( seq_dirs ) self . seq_files = [ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))] self . num_sample_seqs = len ( self . seq_files ) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) ''' # For training, the size of dataset should be 17065 * 2; for validation: 1623; for testing: 4309 if split == 'train' and self.num_sample_seqs != 17065 * 2: warnings.warn(\">> The size of training dataset is not 17065 * 2.\\n\") elif split == 'val' and self.num_sample_seqs != 1623: warnings.warn(\">> The size of validation dataset is not 1719.\\n\") elif split == 'test' and self.num_sample_seqs != 4309: warnings.warn('>> The size of test dataset is not 4309.\\n') ''' # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), \\ int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = manager . dict () self . cache_size = cache_size if split == 'train' else 0 # self.cache_size = cache_size def __len__ ( self ): return self . num_sample_seqs def get_one_hot ( self , label , category_num ): one_hot_label = np . zeros (( label . shape [ 0 ], category_num )) for i in range ( label . shape [ 0 ]): one_hot_label [ i ][ label [ i ]] = 1 return one_hot_label def __getitem__ ( self , idx ): if idx in self . cache : gt_dict = self . cache [ idx ] else : seq_file = self . seq_files [ idx ] gt_data_handle = np . load ( seq_file , allow_pickle = True ) gt_dict = gt_data_handle . item () if len ( self . cache ) < self . cache_size : self . cache [ idx ] = gt_dict allocation_mask = gt_dict [ 'allocation_mask' ] . astype ( np . bool ) reg_loss_mask = gt_dict [ 'reg_loss_mask' ] . astype ( np . bool ) gt_max_iou = gt_dict [ 'gt_max_iou' ] motion_one_hot = np . zeros ( 5 ) motion_mask = np . zeros ( 5 ) # load regression target reg_target_sparse = gt_dict [ 'reg_target_sparse' ] # need to be modified Yiqi , only use reg_target and allocation_map reg_target = np . zeros ( self . reg_target_shape ) . astype ( reg_target_sparse . dtype ) reg_target [ allocation_mask ] = reg_target_sparse reg_target [ np . bitwise_not ( reg_loss_mask )] = 0 label_sparse = gt_dict [ 'label_sparse' ] one_hot_label_sparse = self . get_one_hot ( label_sparse , self . category_num ) label_one_hot = np . zeros ( self . label_one_hot_shape ) label_one_hot [:, :, :, 0 ] = 1 label_one_hot [ allocation_mask ] = one_hot_label_sparse if self . config . motion_state : motion_sparse = gt_dict [ 'motion_state' ] motion_one_hot_label_sparse = self . get_one_hot ( motion_sparse , 3 ) motion_one_hot = np . zeros ( self . label_one_hot_shape [: - 1 ] + ( 3 ,)) motion_one_hot [:, :, :, 0 ] = 1 motion_one_hot [ allocation_mask ] = motion_one_hot_label_sparse motion_mask = ( motion_one_hot [:, :, :, 2 ] == 1 ) if self . only_det : reg_target = reg_target [:, :, :, : 1 ] reg_loss_mask = reg_loss_mask [:, :, :, : 1 ] # only center for pred elif self . config . pred_type in [ 'motion' , 'center' ]: reg_loss_mask = np . expand_dims ( reg_loss_mask , axis =- 1 ) reg_loss_mask = np . repeat ( reg_loss_mask , self . box_code_size , axis =- 1 ) reg_loss_mask [:, :, :, 1 :, 2 :] = False if self . config . use_map : if ( 'map_allocation_0' in gt_dict . keys ()) or ( 'map_allocation' in gt_dict . keys ()): semantic_maps = [] for m_id in range ( self . config . map_channel ): map_alloc = gt_dict [ 'map_allocation_' + str ( m_id )] map_sparse = gt_dict [ 'map_sparse_' + str ( m_id )] recover = np . zeros ( tuple ( self . config . map_dims [: 2 ])) recover [ map_alloc ] = map_sparse recover = np . rot90 ( recover , 3 ) # recover_map = cv2.resize(recover,(self.config.map_dims[0],self.config.map_dims[1])) semantic_maps . append ( recover ) semantic_maps = np . asarray ( semantic_maps ) else : semantic_maps = np . zeros ( 0 ) ''' if self.binary: reg_target = np.concatenate([reg_target[:,:,:2],reg_target[:,:,5:]],axis=2) reg_loss_mask = np.concatenate([reg_loss_mask[:,:,:2],reg_loss_mask[:,:,5:]],axis=2) label_one_hot = np.concatenate([label_one_hot[:,:,:2],label_one_hot[:,:,5:]],axis=2) ''' padded_voxel_points = list () for i in range ( self . num_past_pcs ): indices = gt_dict [ 'voxel_indices_' + str ( i )] curr_voxels = np . zeros ( self . dims , dtype = bool ) curr_voxels [ indices [:, 0 ], indices [:, 1 ], indices [:, 2 ]] = 1 curr_voxels = np . rot90 ( curr_voxels , 3 ) padded_voxel_points . append ( curr_voxels ) padded_voxel_points = np . stack ( padded_voxel_points , 0 ) . astype ( np . float32 ) anchors_map = self . anchors_map ''' if self.binary: anchors_map = np.concatenate([anchors_map[:,:,:2],anchors_map[:,:,5:]],axis=2) ''' if self . config . use_vis : vis_maps = np . zeros ( ( self . num_past_pcs , self . config . map_dims [ - 1 ], self . config . map_dims [ 0 ], self . config . map_dims [ 1 ])) vis_free_indices = gt_dict [ 'vis_free_indices' ] vis_occupy_indices = gt_dict [ 'vis_occupy_indices' ] vis_maps [ vis_occupy_indices [ 0 , :], vis_occupy_indices [ 1 , :], vis_occupy_indices [ 2 , :], vis_occupy_indices [ 3 , :]] = math . log ( 0.7 / ( 1 - 0.7 )) vis_maps [ vis_free_indices [ 0 , :], vis_free_indices [ 1 , :], vis_free_indices [ 2 , :], vis_free_indices [ 3 , :]] = math . log ( 0.4 / ( 1 - 0.4 )) vis_maps = np . swapaxes ( vis_maps , 2 , 3 ) vis_maps = np . transpose ( vis_maps , ( 0 , 2 , 3 , 1 )) for v_id in range ( vis_maps . shape [ 0 ]): vis_maps [ v_id ] = np . rot90 ( vis_maps [ v_id ], 3 ) vis_maps = vis_maps [ - 1 ] else : vis_maps = np . zeros ( 0 ) padded_voxel_points = padded_voxel_points . astype ( np . float32 ) label_one_hot = label_one_hot . astype ( np . float32 ) reg_target = reg_target . astype ( np . float32 ) anchors_map = anchors_map . astype ( np . float32 ) motion_one_hot = motion_one_hot . astype ( np . float32 ) semantic_maps = semantic_maps . astype ( np . float32 ) vis_maps = vis_maps . astype ( np . float32 ) if self . val : return padded_voxel_points , label_one_hot , \\ reg_target , reg_loss_mask , anchors_map , motion_one_hot , motion_mask , vis_maps , [ { \"gt_box\" : gt_max_iou }], [ seq_file ] else : return padded_voxel_points , label_one_hot , \\ reg_target , reg_loss_mask , anchors_map , motion_one_hot , motion_mask , vis_maps","title":"NuscenesDataset"},{"location":"v2x_sim_det_reference/#coperception.datasets.V2XSimDet.NuscenesDataset.__init__","text":"This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) Source code in coperception/datasets/V2XSimDet.py def __init__ ( self , dataset_root = None , config = None , split = None , cache_size = 10000 , val = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis # dataset_root = dataset_root + '/'+split if dataset_root is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_root = dataset_root seq_dirs = [ os . path . join ( self . dataset_root , d ) for d in os . listdir ( self . dataset_root ) if os . path . isdir ( os . path . join ( self . dataset_root , d ))] seq_dirs = sorted ( seq_dirs ) self . seq_files = [ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))] self . num_sample_seqs = len ( self . seq_files ) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) ''' # For training, the size of dataset should be 17065 * 2; for validation: 1623; for testing: 4309 if split == 'train' and self.num_sample_seqs != 17065 * 2: warnings.warn(\">> The size of training dataset is not 17065 * 2.\\n\") elif split == 'val' and self.num_sample_seqs != 1623: warnings.warn(\">> The size of validation dataset is not 1719.\\n\") elif split == 'test' and self.num_sample_seqs != 4309: warnings.warn('>> The size of test dataset is not 4309.\\n') ''' # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), \\ int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = manager . dict () self . cache_size = cache_size if split == 'train' else 0 # self.cache_size = cache_size","title":"__init__()"},{"location":"v2x_sim_det_reference/#coperception.datasets.V2XSimDet.V2XSimDet","text":"Source code in coperception/datasets/V2XSimDet.py class V2XSimDet ( Dataset ): def __init__ ( self , dataset_roots = None , config = None , config_global = None , split = None , cache_size = 10000 , val = False , bound = None , kd_flag = False , no_cross_road = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis self . bound = bound self . kd_flag = kd_flag self . no_cross_road = no_cross_road # dataset_root = dataset_root + '/'+split if dataset_roots is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_roots = dataset_roots self . num_agent = len ( dataset_roots ) self . seq_files = [] self . seq_scenes = [] for dataset_root in self . dataset_roots : # sort directories dir_list = [ d . split ( '_' ) for d in os . listdir ( dataset_root )] dir_list . sort ( key = lambda x : ( int ( x [ 0 ]), int ( x [ 1 ]))) self . seq_scenes . append ([ int ( s [ 0 ]) for s in dir_list ]) # which scene this frame belongs to (required for visualization) dir_list = [ '_' . join ( x ) for x in dir_list ] seq_dirs = [ os . path . join ( dataset_root , d ) for d in dir_list if os . path . isdir ( os . path . join ( dataset_root , d ))] self . seq_files . append ([ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))]) self . num_sample_seqs = len ( self . seq_files [ 0 ]) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = [ manager . dict () for _ in range ( self . num_agent )] self . cache_size = cache_size if split == 'train' else 0 if self . val : self . voxel_size_global = config_global . voxel_size self . area_extents_global = config_global . area_extents self . pred_len_global = config_global . pred_len self . box_code_size_global = config_global . box_code_size self . anchor_size_global = config_global . anchor_size # object information self . anchors_map_global = init_anchors_no_check ( self . area_extents_global , self . voxel_size_global , self . box_code_size_global , self . anchor_size_global ) self . map_dims_global = [ int (( self . area_extents_global [ 0 ][ 1 ] - self . area_extents_global [ 0 ][ 0 ]) / self . voxel_size_global [ 0 ]), \\ int (( self . area_extents_global [ 1 ][ 1 ] - self . area_extents_global [ 1 ][ 0 ]) / self . voxel_size_global [ 1 ])] self . reg_target_shape_global = ( self . map_dims_global [ 0 ], self . map_dims_global [ 1 ], len ( self . anchor_size_global ), self . pred_len_global , self . box_code_size_global ) self . dims_global = config_global . map_dims self . get_meta () def get_meta ( self ): meta = NuscenesDataset ( dataset_root = self . dataset_roots [ 0 ], split = self . split , config = self . config , val = self . val ) if not self . val : self . padded_voxel_points_meta , self . label_one_hot_meta , self . reg_target_meta , self . reg_loss_mask_meta , \\ self . anchors_map_meta , _ , _ , self . vis_maps_meta = meta [ 0 ] else : self . padded_voxel_points_meta , self . label_one_hot_meta , self . reg_target_meta , self . reg_loss_mask_meta , \\ self . anchors_map_meta , _ , _ , self . vis_maps_meta , _ , _ = meta [ 0 ] del meta def __len__ ( self ): return self . num_sample_seqs def get_one_hot ( self , label , category_num ): one_hot_label = np . zeros (( label . shape [ 0 ], category_num )) for i in range ( label . shape [ 0 ]): one_hot_label [ i ][ label [ i ]] = 1 return one_hot_label def pick_single_agent ( self , agent_id , idx ): empty_flag = False if idx in self . cache [ agent_id ]: gt_dict = self . cache [ agent_id ][ idx ] else : seq_file = self . seq_files [ agent_id ][ idx ] gt_data_handle = np . load ( seq_file , allow_pickle = True ) if gt_data_handle == 0 : empty_flag = True padded_voxel_points = [] padded_voxel_points_teacher = [] label_one_hot = np . zeros_like ( self . label_one_hot_meta ) reg_target = np . zeros_like ( self . reg_target_meta ) anchors_map = np . zeros_like ( self . anchors_map_meta ) vis_maps = np . zeros_like ( self . vis_maps_meta ) reg_loss_mask = np . zeros_like ( self . reg_loss_mask_meta ) if self . bound == 'lowerbound' : padded_voxel_points = np . zeros_like ( self . padded_voxel_points_meta ) if self . kd_flag or self . bound == 'upperbound' : padded_voxel_points_teacher = np . zeros_like ( self . padded_voxel_points_meta ) if self . val : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , \\ reg_target , reg_loss_mask , anchors_map , vis_maps , [ { \"gt_box\" : []}], [ seq_file ], \\ 0 , 0 , np . zeros (( self . num_agent , 4 , 4 )) else : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , reg_target , reg_loss_mask , anchors_map , vis_maps , 0 , 0 , np . zeros ( ( self . num_agent , 4 , 4 )) else : gt_dict = gt_data_handle . item () if len ( self . cache [ agent_id ]) < self . cache_size : self . cache [ agent_id ][ idx ] = gt_dict if empty_flag == False : allocation_mask = gt_dict [ 'allocation_mask' ] . astype ( bool ) reg_loss_mask = gt_dict [ 'reg_loss_mask' ] . astype ( bool ) gt_max_iou = gt_dict [ 'gt_max_iou' ] # load regression target reg_target_sparse = gt_dict [ 'reg_target_sparse' ] # need to be modified Yiqi , only use reg_target and allocation_map reg_target = np . zeros ( self . reg_target_shape ) . astype ( reg_target_sparse . dtype ) reg_target [ allocation_mask ] = reg_target_sparse reg_target [ np . bitwise_not ( reg_loss_mask )] = 0 label_sparse = gt_dict [ 'label_sparse' ] one_hot_label_sparse = self . get_one_hot ( label_sparse , self . category_num ) label_one_hot = np . zeros ( self . label_one_hot_shape ) label_one_hot [:, :, :, 0 ] = 1 label_one_hot [ allocation_mask ] = one_hot_label_sparse if self . only_det : reg_target = reg_target [:, :, :, : 1 ] reg_loss_mask = reg_loss_mask [:, :, :, : 1 ] # only center for pred elif self . config . pred_type in [ 'motion' , 'center' ]: reg_loss_mask = np . expand_dims ( reg_loss_mask , axis =- 1 ) reg_loss_mask = np . repeat ( reg_loss_mask , self . box_code_size , axis =- 1 ) reg_loss_mask [:, :, :, 1 :, 2 :] = False # Prepare padded_voxel_points padded_voxel_points = [] if self . bound == 'lowerbound' or self . bound == 'both' : for i in range ( self . num_past_pcs ): indices = gt_dict [ 'voxel_indices_' + str ( i )] curr_voxels = np . zeros ( self . dims , dtype = bool ) curr_voxels [ indices [:, 0 ], indices [:, 1 ], indices [:, 2 ]] = 1 curr_voxels = np . rot90 ( curr_voxels , 3 ) padded_voxel_points . append ( curr_voxels ) padded_voxel_points = np . stack ( padded_voxel_points , 0 ) . astype ( np . float32 ) padded_voxel_points = padded_voxel_points . astype ( np . float32 ) anchors_map = self . anchors_map if self . config . use_vis : vis_maps = np . zeros ( ( self . num_past_pcs , self . config . map_dims [ - 1 ], self . config . map_dims [ 0 ], self . config . map_dims [ 1 ])) vis_free_indices = gt_dict [ 'vis_free_indices' ] vis_occupy_indices = gt_dict [ 'vis_occupy_indices' ] vis_maps [ vis_occupy_indices [ 0 , :], vis_occupy_indices [ 1 , :], vis_occupy_indices [ 2 , :], vis_occupy_indices [ 3 , :]] = math . log ( 0.7 / ( 1 - 0.7 )) vis_maps [ vis_free_indices [ 0 , :], vis_free_indices [ 1 , :], vis_free_indices [ 2 , :], vis_free_indices [ 3 , :]] = math . log ( 0.4 / ( 1 - 0.4 )) vis_maps = np . swapaxes ( vis_maps , 2 , 3 ) vis_maps = np . transpose ( vis_maps , ( 0 , 2 , 3 , 1 )) for v_id in range ( vis_maps . shape [ 0 ]): vis_maps [ v_id ] = np . rot90 ( vis_maps [ v_id ], 3 ) vis_maps = vis_maps [ - 1 ] else : vis_maps = np . zeros ( 0 ) if self . no_cross_road : trans_matrices = gt_dict [ 'trans_matrices_no_cross_road' ] else : trans_matrices = gt_dict [ 'trans_matrices' ] label_one_hot = label_one_hot . astype ( np . float32 ) reg_target = reg_target . astype ( np . float32 ) anchors_map = anchors_map . astype ( np . float32 ) vis_maps = vis_maps . astype ( np . float32 ) target_agent_id = gt_dict [ 'target_agent_id' ] num_sensor = gt_dict [ 'num_sensor' ] # Prepare padded_voxel_points_teacher padded_voxel_points_teacher = [] if 'voxel_indices_teacher' in gt_dict and ( self . kd_flag or self . bound == 'upperbound' or self . bound == 'both' ): if self . no_cross_road : indices_teacher = gt_dict [ 'voxel_indices_teacher_no_cross_road' ] else : indices_teacher = gt_dict [ 'voxel_indices_teacher' ] curr_voxels_teacher = np . zeros ( self . dims , dtype = bool ) curr_voxels_teacher [ indices_teacher [:, 0 ], indices_teacher [:, 1 ], indices_teacher [:, 2 ]] = 1 curr_voxels_teacher = np . rot90 ( curr_voxels_teacher , 3 ) padded_voxel_points_teacher . append ( curr_voxels_teacher ) padded_voxel_points_teacher = np . stack ( padded_voxel_points_teacher , 0 ) . astype ( np . float32 ) padded_voxel_points_teacher = padded_voxel_points_teacher . astype ( np . float32 ) if self . val : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , reg_target , reg_loss_mask , anchors_map , vis_maps , [ { \"gt_box\" : gt_max_iou }], [ seq_file ], \\ target_agent_id , num_sensor , trans_matrices else : return padded_voxel_points , padded_voxel_points_teacher , label_one_hot , reg_target , reg_loss_mask , anchors_map , vis_maps , \\ target_agent_id , num_sensor , trans_matrices def __getitem__ ( self , idx ): res = [] for i in range ( self . num_agent ): res . append ( self . pick_single_agent ( i , idx )) return res","title":"V2XSimDet"},{"location":"v2x_sim_det_reference/#coperception.datasets.V2XSimDet.V2XSimDet.__init__","text":"This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) Source code in coperception/datasets/V2XSimDet.py def __init__ ( self , dataset_roots = None , config = None , config_global = None , split = None , cache_size = 10000 , val = False , bound = None , kd_flag = False , no_cross_road = False ): \"\"\" This dataloader loads single sequence for a keyframe, and is not designed for computing the spatio-temporal consistency losses. It supports train, val and test splits. dataset_root: Data path to the preprocessed sparse nuScenes data (for training) split: [train/val/test] future_frame_skip: Specify to skip how many future frames voxel_size: The lattice resolution. Should be consistent with the preprocessed data area_extents: The area extents of the processed LiDAR data. Should be consistent with the preprocessed data category_num: The number of object categories (including the background) cache_size: The cache size for storing parts of data in the memory (for reducing the IO cost) \"\"\" if split is None : self . split = config . split else : self . split = split self . voxel_size = config . voxel_size self . area_extents = config . area_extents self . category_num = config . category_num self . future_frame_skip = config . future_frame_skip self . pred_len = config . pred_len self . box_code_size = config . box_code_size self . anchor_size = config . anchor_size self . val = val self . only_det = config . only_det self . binary = config . binary self . config = config self . use_vis = config . use_vis self . bound = bound self . kd_flag = kd_flag self . no_cross_road = no_cross_road # dataset_root = dataset_root + '/'+split if dataset_roots is None : raise ValueError ( \"The {} dataset root is None. Should specify its value.\" . format ( self . split )) self . dataset_roots = dataset_roots self . num_agent = len ( dataset_roots ) self . seq_files = [] self . seq_scenes = [] for dataset_root in self . dataset_roots : # sort directories dir_list = [ d . split ( '_' ) for d in os . listdir ( dataset_root )] dir_list . sort ( key = lambda x : ( int ( x [ 0 ]), int ( x [ 1 ]))) self . seq_scenes . append ([ int ( s [ 0 ]) for s in dir_list ]) # which scene this frame belongs to (required for visualization) dir_list = [ '_' . join ( x ) for x in dir_list ] seq_dirs = [ os . path . join ( dataset_root , d ) for d in dir_list if os . path . isdir ( os . path . join ( dataset_root , d ))] self . seq_files . append ([ os . path . join ( seq_dir , f ) for seq_dir in seq_dirs for f in os . listdir ( seq_dir ) if os . path . isfile ( os . path . join ( seq_dir , f ))]) self . num_sample_seqs = len ( self . seq_files [ 0 ]) print ( \"The number of {} sequences: {} \" . format ( self . split , self . num_sample_seqs )) # object information self . anchors_map = init_anchors_no_check ( self . area_extents , self . voxel_size , self . box_code_size , self . anchor_size ) self . map_dims = [ int (( self . area_extents [ 0 ][ 1 ] - self . area_extents [ 0 ][ 0 ]) / self . voxel_size [ 0 ]), int (( self . area_extents [ 1 ][ 1 ] - self . area_extents [ 1 ][ 0 ]) / self . voxel_size [ 1 ])] self . reg_target_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . pred_len , self . box_code_size ) self . label_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size )) self . label_one_hot_shape = ( self . map_dims [ 0 ], self . map_dims [ 1 ], len ( self . anchor_size ), self . category_num ) self . dims = config . map_dims self . num_past_pcs = config . num_past_pcs manager = Manager () self . cache = [ manager . dict () for _ in range ( self . num_agent )] self . cache_size = cache_size if split == 'train' else 0 if self . val : self . voxel_size_global = config_global . voxel_size self . area_extents_global = config_global . area_extents self . pred_len_global = config_global . pred_len self . box_code_size_global = config_global . box_code_size self . anchor_size_global = config_global . anchor_size # object information self . anchors_map_global = init_anchors_no_check ( self . area_extents_global , self . voxel_size_global , self . box_code_size_global , self . anchor_size_global ) self . map_dims_global = [ int (( self . area_extents_global [ 0 ][ 1 ] - self . area_extents_global [ 0 ][ 0 ]) / self . voxel_size_global [ 0 ]), \\ int (( self . area_extents_global [ 1 ][ 1 ] - self . area_extents_global [ 1 ][ 0 ]) / self . voxel_size_global [ 1 ])] self . reg_target_shape_global = ( self . map_dims_global [ 0 ], self . map_dims_global [ 1 ], len ( self . anchor_size_global ), self . pred_len_global , self . box_code_size_global ) self . dims_global = config_global . map_dims self . get_meta ()","title":"__init__()"},{"location":"test/","text":"Title # This is a comment print(\"Hello, world!\")","title":"Title"},{"location":"test/#title","text":"# This is a comment print(\"Hello, world!\")","title":"Title"}]}